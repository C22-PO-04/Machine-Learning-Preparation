{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd64eebc",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modelling</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Utilities\" data-toc-modified-id=\"Utilities-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Utilities</a></span></li></ul></li><li><span><a href=\"#Optimizing-top-10-Category\" data-toc-modified-id=\"Optimizing-top-10-Category-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Optimizing top 10 Category</a></span><ul class=\"toc-item\"><li><span><a href=\"#Training-Model\" data-toc-modified-id=\"Training-Model-1.1.1\"><span class=\"toc-item-num\">1.1.1&nbsp;&nbsp;</span>Training Model</a></span></li><li><span><a href=\"#Price-Optimization-Demonstration\" data-toc-modified-id=\"Price-Optimization-Demonstration-1.1.2\"><span class=\"toc-item-num\">1.1.2&nbsp;&nbsp;</span>Price Optimization Demonstration</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89624cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('display.max_row', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e4a283",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>sales</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>2</td>\n",
       "      <td>111285.5364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephony</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>181684.4400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37802</th>\n",
       "      <td>kitchen_dining_laundry_garden_furniture</td>\n",
       "      <td>2018-08-30</td>\n",
       "      <td>1</td>\n",
       "      <td>442760.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37803</th>\n",
       "      <td>kitchen_dining_laundry_garden_furniture</td>\n",
       "      <td>2018-08-31</td>\n",
       "      <td>1</td>\n",
       "      <td>442760.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37804</th>\n",
       "      <td>kitchen_dining_laundry_garden_furniture</td>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>1</td>\n",
       "      <td>442760.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37805</th>\n",
       "      <td>kitchen_dining_laundry_garden_furniture</td>\n",
       "      <td>2018-09-02</td>\n",
       "      <td>1</td>\n",
       "      <td>442760.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37806</th>\n",
       "      <td>kitchen_dining_laundry_garden_furniture</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>1</td>\n",
       "      <td>442760.4000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37807 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         product_category_name order_purchase_timestamp  \\\n",
       "0                              furniture_decor               2016-09-04   \n",
       "1                                    telephony               2016-09-05   \n",
       "2                              furniture_decor               2016-09-05   \n",
       "3                              furniture_decor               2016-09-06   \n",
       "4                              furniture_decor               2016-09-07   \n",
       "...                                        ...                      ...   \n",
       "37802  kitchen_dining_laundry_garden_furniture               2018-08-30   \n",
       "37803  kitchen_dining_laundry_garden_furniture               2018-08-31   \n",
       "37804  kitchen_dining_laundry_garden_furniture               2018-09-01   \n",
       "37805  kitchen_dining_laundry_garden_furniture               2018-09-02   \n",
       "37806  kitchen_dining_laundry_garden_furniture               2018-09-03   \n",
       "\n",
       "       sales        price  \n",
       "0          2  111285.5364  \n",
       "1          1  181684.4400  \n",
       "2          1  122110.2648  \n",
       "3          1  122110.2648  \n",
       "4          1  122110.2648  \n",
       "...      ...          ...  \n",
       "37802      1  442760.4000  \n",
       "37803      1  442760.4000  \n",
       "37804      1  442760.4000  \n",
       "37805      1  442760.4000  \n",
       "37806      1  442760.4000  \n",
       "\n",
       "[37807 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../data/prepared/interpolated_demand_history_by_category.csv', index_col=0)\n",
    "data['order_purchase_timestamp'] = pd.to_datetime(data['order_purchase_timestamp'])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6db482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categories = ['bed_bath_table', 'health_beauty', 'sports_leisure', 'furniture_decor','computers_accessories', 'housewares', 'watches_gifts', 'telephony','garden_tools', 'auto']\n",
    "data = data[data.product_category_name.apply(lambda x: x in selected_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb63570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manfred/miniconda3/envs/tensorflow/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data[['sales', 'price']])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoded_category = encoder.fit_transform(data[['product_category_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c9021f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/demand_forecasting/encoder.joblib']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(scaler, \"../model/demand_forecasting/scaler.joblib\")\n",
    "joblib.dump(encoder, \"../model/demand_forecasting/encoder.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c15471c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1863/4205915431.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['scaled_sales'] = scaled_data[:,0]\n",
      "/tmp/ipykernel_1863/4205915431.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['scaled_price'] = scaled_data[:,1]\n",
      "/tmp/ipykernel_1863/4205915431.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['encoded_category'] = encoded_category\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>sales</th>\n",
       "      <th>price</th>\n",
       "      <th>scaled_sales</th>\n",
       "      <th>scaled_price</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>2</td>\n",
       "      <td>111285.5364</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.775227</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephony</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>181684.4400</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.598523</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37791</th>\n",
       "      <td>computers_accessories</td>\n",
       "      <td>2018-08-28</td>\n",
       "      <td>4</td>\n",
       "      <td>281595.6144</td>\n",
       "      <td>-1.219400</td>\n",
       "      <td>-0.347743</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37795</th>\n",
       "      <td>watches_gifts</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>2</td>\n",
       "      <td>410393.0880</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.024456</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37796</th>\n",
       "      <td>computers_accessories</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>259518.6648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.403157</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37798</th>\n",
       "      <td>health_beauty</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>3</td>\n",
       "      <td>206061.7080</td>\n",
       "      <td>-1.226328</td>\n",
       "      <td>-0.537336</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37801</th>\n",
       "      <td>sports_leisure</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>21069.2880</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-1.001673</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6971 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_category_name order_purchase_timestamp  sales        price  \\\n",
       "0            furniture_decor               2016-09-04      2  111285.5364   \n",
       "1                  telephony               2016-09-05      1  181684.4400   \n",
       "2            furniture_decor               2016-09-05      1  122110.2648   \n",
       "3            furniture_decor               2016-09-06      1  122110.2648   \n",
       "4            furniture_decor               2016-09-07      1  122110.2648   \n",
       "...                      ...                      ...    ...          ...   \n",
       "37791  computers_accessories               2018-08-28      4  281595.6144   \n",
       "37795          watches_gifts               2018-08-29      2  410393.0880   \n",
       "37796  computers_accessories               2018-08-29      1  259518.6648   \n",
       "37798          health_beauty               2018-08-29      3  206061.7080   \n",
       "37801         sports_leisure               2018-08-29      1   21069.2880   \n",
       "\n",
       "       scaled_sales  scaled_price  encoded_category  \n",
       "0         -1.233255     -0.775227                 3  \n",
       "1         -1.240183     -0.598523                 8  \n",
       "2         -1.240183     -0.748057                 3  \n",
       "3         -1.240183     -0.748057                 3  \n",
       "4         -1.240183     -0.748057                 3  \n",
       "...             ...           ...               ...  \n",
       "37791     -1.219400     -0.347743                 2  \n",
       "37795     -1.233255     -0.024456                 9  \n",
       "37796     -1.240183     -0.403157                 2  \n",
       "37798     -1.226328     -0.537336                 5  \n",
       "37801     -1.240183     -1.001673                 7  \n",
       "\n",
       "[6971 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['scaled_sales'] = scaled_data[:,0]\n",
    "data['scaled_price'] = scaled_data[:,1]\n",
    "data['encoded_category'] = encoded_category\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cb574f",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301d2a82",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cf47777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wandb\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67443e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(ds, n_sample=3):\n",
    "    for x in ds.take(n_sample):\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d56b5f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history):\n",
    "    fig = plt.figure(figsize=(14, 8))\n",
    "    plt.plot(pd.DataFrame(history.history)[['loss', 'val_loss']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb319d6",
   "metadata": {},
   "source": [
    "## Optimizing top 10 Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6aed8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmanfredmichael\u001b[0m (\u001b[33manakbangkit\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-06-15 06:53:26.833581: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-15 06:53:26.833628: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.18 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/manfred/Projects/bangkit/CompanyBasedCapstone/Modelling/wandb/run-20220615_065322-2oiu85vz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/anakbangkit/cateogory-discount-optimization/runs/2oiu85vz\" target=\"_blank\">noble-pond-37</a></strong> to <a href=\"https://wandb.ai/anakbangkit/cateogory-discount-optimization\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"cateogory-discount-optimization\", entity=\"anakbangkit\", reinit=True)\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c862f",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce367734",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.test_size = 0.2\n",
    "config.valid_size = 0.2\n",
    "config.window_size = 30\n",
    "config.batch_size = 8\n",
    "config.buffer_size = 100_000\n",
    "config.stateful = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21e28ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>sales</th>\n",
       "      <th>price</th>\n",
       "      <th>scaled_sales</th>\n",
       "      <th>scaled_price</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>2</td>\n",
       "      <td>111285.5364</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.775227</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephony</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>181684.4400</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.598523</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37784</th>\n",
       "      <td>watches_gifts</td>\n",
       "      <td>2018-08-28</td>\n",
       "      <td>2</td>\n",
       "      <td>241228.0800</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.449067</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37796</th>\n",
       "      <td>computers_accessories</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>259518.6648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.403157</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37798</th>\n",
       "      <td>health_beauty</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>3</td>\n",
       "      <td>206061.7080</td>\n",
       "      <td>-1.226328</td>\n",
       "      <td>-0.537336</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37795</th>\n",
       "      <td>watches_gifts</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>2</td>\n",
       "      <td>410393.0880</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.024456</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37801</th>\n",
       "      <td>sports_leisure</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>21069.2880</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-1.001673</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6971 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_category_name order_purchase_timestamp  sales        price  \\\n",
       "0            furniture_decor               2016-09-04      2  111285.5364   \n",
       "1                  telephony               2016-09-05      1  181684.4400   \n",
       "2            furniture_decor               2016-09-05      1  122110.2648   \n",
       "3            furniture_decor               2016-09-06      1  122110.2648   \n",
       "4            furniture_decor               2016-09-07      1  122110.2648   \n",
       "...                      ...                      ...    ...          ...   \n",
       "37784          watches_gifts               2018-08-28      2  241228.0800   \n",
       "37796  computers_accessories               2018-08-29      1  259518.6648   \n",
       "37798          health_beauty               2018-08-29      3  206061.7080   \n",
       "37795          watches_gifts               2018-08-29      2  410393.0880   \n",
       "37801         sports_leisure               2018-08-29      1   21069.2880   \n",
       "\n",
       "       scaled_sales  scaled_price  encoded_category  \n",
       "0         -1.233255     -0.775227                 3  \n",
       "1         -1.240183     -0.598523                 8  \n",
       "2         -1.240183     -0.748057                 3  \n",
       "3         -1.240183     -0.748057                 3  \n",
       "4         -1.240183     -0.748057                 3  \n",
       "...             ...           ...               ...  \n",
       "37784     -1.233255     -0.449067                 9  \n",
       "37796     -1.240183     -0.403157                 2  \n",
       "37798     -1.226328     -0.537336                 5  \n",
       "37795     -1.233255     -0.024456                 9  \n",
       "37801     -1.240183     -1.001673                 7  \n",
       "\n",
       "[6971 rows x 7 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.sort_values('order_purchase_timestamp')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5f6e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_categories = ['bed_bath_table', 'health_beauty', 'sports_leisure', 'furniture_decor','computers_accessories', 'housewares', 'watches_gifts', 'telephony','garden_tools', 'auto']\n",
    "data = data[data.product_category_name.apply(lambda x: x in selected_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "401fe7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>sales</th>\n",
       "      <th>price</th>\n",
       "      <th>scaled_sales</th>\n",
       "      <th>scaled_price</th>\n",
       "      <th>encoded_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-04</td>\n",
       "      <td>2</td>\n",
       "      <td>111285.5364</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.775227</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>telephony</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>181684.4400</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.598523</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-05</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-06</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>2016-09-07</td>\n",
       "      <td>1</td>\n",
       "      <td>122110.2648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.748057</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37784</th>\n",
       "      <td>watches_gifts</td>\n",
       "      <td>2018-08-28</td>\n",
       "      <td>2</td>\n",
       "      <td>241228.0800</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.449067</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37796</th>\n",
       "      <td>computers_accessories</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>259518.6648</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-0.403157</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37798</th>\n",
       "      <td>health_beauty</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>3</td>\n",
       "      <td>206061.7080</td>\n",
       "      <td>-1.226328</td>\n",
       "      <td>-0.537336</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37795</th>\n",
       "      <td>watches_gifts</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>2</td>\n",
       "      <td>410393.0880</td>\n",
       "      <td>-1.233255</td>\n",
       "      <td>-0.024456</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37801</th>\n",
       "      <td>sports_leisure</td>\n",
       "      <td>2018-08-29</td>\n",
       "      <td>1</td>\n",
       "      <td>21069.2880</td>\n",
       "      <td>-1.240183</td>\n",
       "      <td>-1.001673</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6971 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_category_name order_purchase_timestamp  sales        price  \\\n",
       "0            furniture_decor               2016-09-04      2  111285.5364   \n",
       "1                  telephony               2016-09-05      1  181684.4400   \n",
       "2            furniture_decor               2016-09-05      1  122110.2648   \n",
       "3            furniture_decor               2016-09-06      1  122110.2648   \n",
       "4            furniture_decor               2016-09-07      1  122110.2648   \n",
       "...                      ...                      ...    ...          ...   \n",
       "37784          watches_gifts               2018-08-28      2  241228.0800   \n",
       "37796  computers_accessories               2018-08-29      1  259518.6648   \n",
       "37798          health_beauty               2018-08-29      3  206061.7080   \n",
       "37795          watches_gifts               2018-08-29      2  410393.0880   \n",
       "37801         sports_leisure               2018-08-29      1   21069.2880   \n",
       "\n",
       "       scaled_sales  scaled_price  encoded_category  \n",
       "0         -1.233255     -0.775227                 3  \n",
       "1         -1.240183     -0.598523                 8  \n",
       "2         -1.240183     -0.748057                 3  \n",
       "3         -1.240183     -0.748057                 3  \n",
       "4         -1.240183     -0.748057                 3  \n",
       "...             ...           ...               ...  \n",
       "37784     -1.233255     -0.449067                 9  \n",
       "37796     -1.240183     -0.403157                 2  \n",
       "37798     -1.226328     -0.537336                 5  \n",
       "37795     -1.233255     -0.024456                 9  \n",
       "37801     -1.240183     -1.001673                 7  \n",
       "\n",
       "[6971 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "83e874f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(data):\n",
    "    train_split_time = int((1 - (config.valid_size + config.test_size)) * len(data))\n",
    "    valid_split_time = int((1 - config.test_size) * len(data))\n",
    "    \n",
    "    train = data.iloc[:train_split_time]\n",
    "    valid = data.iloc[train_split_time:valid_split_time]\n",
    "    test = data.iloc[valid_split_time:]\n",
    "\n",
    "    return train, valid, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc3fe168",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    def __init__(self, window_size, batch_size, buffer_size, stateful):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.window_size = window_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_size = buffer_size\n",
    "        self.stateful = stateful\n",
    "#         scaled_data = scaler.fit_transform(data[['sales', 'price']])\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.scaler.fit(data[['sales', 'price']])\n",
    "        \n",
    "    def rescale(self, data):\n",
    "        data = data.copy()\n",
    "        scaled_data = self.scaler.transform(data[['sales', 'price']])\n",
    "        data['scaled_sales'] = scaled_data[:,0]\n",
    "        data['scaled_price'] = scaled_data[:,1]\n",
    "        return data\n",
    "    \n",
    "    def train_val_split(self, data):\n",
    "        train_split_time = int((1 - (config.valid_size + config.test_size)) * len(data))\n",
    "        valid_split_time = int((1 - config.test_size) * len(data))\n",
    "\n",
    "        train = data.iloc[:train_split_time]\n",
    "        valid = data.iloc[train_split_time:valid_split_time]\n",
    "        test = data.iloc[valid_split_time:]\n",
    "\n",
    "        return train, valid, test\n",
    "    \n",
    "    def create_windowed_dataset(self, data, with_label=True, rescale=True):\n",
    "        if rescale:\n",
    "            data = self.rescale(data)\n",
    "        \n",
    "        data = data[['scaled_sales', 'scaled_price', 'encoded_category', 'sales']].values\n",
    "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
    "        ds = ds.window(self.window_size + 1, shift=1, drop_remainder=True)\n",
    "        ds = ds.flat_map(lambda w: w.batch(self.window_size + 1))\n",
    "\n",
    "        if not self.stateful:\n",
    "            ds = ds.shuffle(self.buffer_size)\n",
    "\n",
    "        if with_label:\n",
    "            ds = ds.map(lambda w: ((w[:-1, :2], w[-1:, 1], w[-1, 2]), w[-1:, 3]))\n",
    "        else:\n",
    "            ds = ds.map(lambda w: (w[:-1, :2], w[-1:, 1], w[-1, 2]))\n",
    "\n",
    "        ds = ds.batch(self.batch_size, drop_remainder=self.stateful)\n",
    "        if not with_label:\n",
    "            ds = ds.prefetch(1)\n",
    "\n",
    "        return ds\n",
    "    \n",
    "    def preprocess(self, data):\n",
    "        train_datasets_list = []\n",
    "        valid_datasets_list = []\n",
    "        test_datasets_list = []\n",
    "        for category, group in tqdm(data.groupby('product_category_name')):\n",
    "            train, valid, test = self.train_val_split(group)\n",
    "            train_datasets_list.append(self.create_windowed_dataset(train))\n",
    "            valid_datasets_list.append(self.create_windowed_dataset(valid))\n",
    "            test_datasets_list.append(self.create_windowed_dataset(test))\n",
    "            \n",
    "        train_datasets = train_datasets_list[0]\n",
    "        valid_datasets = valid_datasets_list[0]\n",
    "        test_datasets = test_datasets_list[0]\n",
    "        for train, valid, test in zip(train_datasets_list[1:], valid_datasets_list[1:], test_datasets_list[1:]):\n",
    "            train_datasets = train_datasets.concatenate(train)\n",
    "            valid_datasets = valid_datasets.concatenate(valid)\n",
    "            test_datasets = test_datasets.concatenate(test)\n",
    "            \n",
    "        return train_datasets, valid_datasets, test_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "873ef41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joblib.dump(DataPreprocessing, \"../model/demand_forecasting/DataPreprocessing.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4ef2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataPreprocessing = joblib.load(\"../model/demand_forecasting/DataPreprocessing.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d3458bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline = DataPreprocessing(\n",
    "    window_size = config.window_size,\n",
    "    batch_size = config.batch_size,\n",
    "    buffer_size = config.buffer_size,\n",
    "    stateful = config.stateful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e5abffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c7b96287",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = train_val_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71d656fb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:05<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "train_set, valid_set, test_set = data_pipeline.preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a31eca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<tf.Tensor: shape=(8, 30, 2), dtype=float64, numpy=\n",
      "array([[[-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01]],\n",
      "\n",
      "       [[-8.86876475e-01,  2.43826071e-01],\n",
      "        [-8.79948901e-01,  2.77158692e-01],\n",
      "        [-8.73021328e-01,  2.66406825e-01],\n",
      "        [-8.73021328e-01,  1.53552075e-01],\n",
      "        [-8.79948901e-01,  1.58993137e-01],\n",
      "        [-8.86876475e-01,  1.45483823e-01],\n",
      "        [-8.86876475e-01,  1.46589272e-01],\n",
      "        [-8.79948901e-01,  1.56955551e-01],\n",
      "        [-8.79948901e-01,  1.47698921e-01],\n",
      "        [-8.79948901e-01,  1.47698921e-01],\n",
      "        [-8.73021328e-01,  1.32886450e-01],\n",
      "        [-8.66093754e-01,  1.58955485e-01],\n",
      "        [-8.66093754e-01,  1.32171721e-01],\n",
      "        [-8.59166181e-01,  1.07407936e-01],\n",
      "        [-8.52238608e-01,  8.77607749e-02],\n",
      "        [-8.66093754e-01,  1.22974382e-01],\n",
      "        [-8.52238608e-01,  9.42836242e-02],\n",
      "        [-8.45311034e-01,  1.79426099e-01],\n",
      "        [-8.38383461e-01,  8.40607355e-02],\n",
      "        [-8.45311034e-01,  9.74944626e-02],\n",
      "        [-8.52238608e-01,  1.09651516e-01],\n",
      "        [-8.59166181e-01,  1.22230433e-01],\n",
      "        [-8.59166181e-01,  1.22230433e-01],\n",
      "        [-8.52238608e-01,  1.19427050e-01],\n",
      "        [-8.52238608e-01,  1.15569278e-01],\n",
      "        [-8.38383461e-01,  2.33737028e-01],\n",
      "        [-8.45311034e-01,  2.53968149e-01],\n",
      "        [-8.38383461e-01,  2.34381361e-01],\n",
      "        [-8.17600740e-01,  2.20718423e-01],\n",
      "        [-8.17600740e-01,  1.95785477e-01]],\n",
      "\n",
      "       [[-6.02845965e-01, -6.93953862e-02],\n",
      "        [-5.68208098e-01, -1.22637757e-01],\n",
      "        [-5.82063245e-01, -1.17761252e-01],\n",
      "        [-5.75135671e-01, -1.21414606e-01],\n",
      "        [-5.54352951e-01, -1.40310443e-01],\n",
      "        [-5.54352951e-01, -1.41383466e-01],\n",
      "        [-5.68208098e-01, -1.30091043e-01],\n",
      "        [-5.47425377e-01, -1.46470399e-01],\n",
      "        [-5.54352951e-01, -1.52474690e-01],\n",
      "        [-5.54352951e-01, -1.44719034e-01],\n",
      "        [-5.54352951e-01, -1.42419700e-01],\n",
      "        [-5.47425377e-01, -1.49782048e-01],\n",
      "        [-5.40497804e-01, -1.46036117e-01],\n",
      "        [-5.40497804e-01, -1.04325736e-01],\n",
      "        [-5.33570231e-01, -1.47571772e-01],\n",
      "        [-5.26642657e-01, -1.58651086e-01],\n",
      "        [-5.40497804e-01, -1.46937817e-01],\n",
      "        [-5.33570231e-01, -1.40874681e-01],\n",
      "        [-5.33570231e-01, -1.54410246e-01],\n",
      "        [-5.26642657e-01, -1.52607963e-01],\n",
      "        [-5.33570231e-01, -1.48754925e-01],\n",
      "        [-5.12787510e-01, -7.15064839e-02],\n",
      "        [-5.33570231e-01, -1.24340307e-01],\n",
      "        [-5.19715084e-01, -1.17240525e-01],\n",
      "        [-4.98932364e-01, -1.22853497e-01],\n",
      "        [-5.12787510e-01, -1.26814159e-01],\n",
      "        [-5.19715084e-01, -1.32132184e-01],\n",
      "        [-5.19715084e-01, -1.22642867e-01],\n",
      "        [-5.33570231e-01, -1.10046482e-01],\n",
      "        [-5.26642657e-01, -1.07620596e-01]],\n",
      "\n",
      "       [[-5.12787510e-01, -1.26814159e-01],\n",
      "        [-5.19715084e-01, -1.32132184e-01],\n",
      "        [-5.19715084e-01, -1.22642867e-01],\n",
      "        [-5.33570231e-01, -1.10046482e-01],\n",
      "        [-5.26642657e-01, -1.07620596e-01],\n",
      "        [-5.26642657e-01, -1.06997860e-01],\n",
      "        [-5.40497804e-01, -1.05242464e-01],\n",
      "        [-5.33570231e-01, -1.03304743e-01],\n",
      "        [-5.12787510e-01, -1.30754842e-01],\n",
      "        [-5.19715084e-01, -1.23409312e-01],\n",
      "        [-4.98932364e-01, -1.06076870e-01],\n",
      "        [-5.19715084e-01, -1.26132746e-01],\n",
      "        [-5.12787510e-01, -1.22606666e-01],\n",
      "        [-4.92004790e-01, -1.22164530e-01],\n",
      "        [-5.12787510e-01, -1.26952263e-01],\n",
      "        [-5.12787510e-01, -1.28542998e-01],\n",
      "        [-5.12787510e-01, -1.18305174e-01],\n",
      "        [-4.78149643e-01, -1.31278560e-01],\n",
      "        [-4.98932364e-01, -1.43663895e-01],\n",
      "        [-4.98932364e-01, -1.79153132e-01],\n",
      "        [-5.12787510e-01, -1.87466702e-01],\n",
      "        [-4.98932364e-01, -1.52860524e-01],\n",
      "        [-4.98932364e-01, -1.97073039e-01],\n",
      "        [-4.71222070e-01, -1.11369649e-01],\n",
      "        [-4.78149643e-01, -1.02565878e-01],\n",
      "        [-4.78149643e-01, -1.13166984e-01],\n",
      "        [-4.85077217e-01, -1.09542001e-01],\n",
      "        [-4.85077217e-01, -8.02087622e-02],\n",
      "        [-4.64294497e-01, -5.86070528e-02],\n",
      "        [-4.64294497e-01, -5.83696583e-02]],\n",
      "\n",
      "       [[-3.67308469e-01, -2.01626239e-01],\n",
      "        [-3.74236042e-01, -1.43652338e-01],\n",
      "        [-3.81163616e-01, -1.48958701e-01],\n",
      "        [-3.81163616e-01, -1.50251847e-01],\n",
      "        [-3.95018762e-01, -1.62896677e-01],\n",
      "        [-3.81163616e-01, -1.69198977e-01],\n",
      "        [-3.74236042e-01, -1.51967048e-01],\n",
      "        [-3.67308469e-01, -1.53045100e-01],\n",
      "        [-3.67308469e-01, -1.48362544e-01],\n",
      "        [-3.46525749e-01, -1.47379188e-01],\n",
      "        [-3.53453322e-01, -1.37370135e-01],\n",
      "        [-3.67308469e-01, -1.34416265e-01],\n",
      "        [-3.60380895e-01, -1.45014355e-01],\n",
      "        [-3.67308469e-01, -1.51742747e-01],\n",
      "        [-3.60380895e-01, -1.49221419e-01],\n",
      "        [-3.67308469e-01, -1.50535747e-01],\n",
      "        [-3.60380895e-01, -1.53406926e-01],\n",
      "        [-3.39598175e-01, -1.62114349e-01],\n",
      "        [-3.39598175e-01, -1.17058507e-01],\n",
      "        [-3.32670602e-01, -1.35772997e-01],\n",
      "        [-3.25743028e-01, -1.64815853e-01],\n",
      "        [-3.25743028e-01, -1.61843430e-01],\n",
      "        [-3.11887881e-01, -1.79163351e-01],\n",
      "        [-2.77250014e-01, -1.54394306e-01],\n",
      "        [-3.11887881e-01, -1.79372846e-01],\n",
      "        [-3.11887881e-01, -1.65697198e-01],\n",
      "        [-3.18815455e-01, -1.67304907e-01],\n",
      "        [-2.91105161e-01, -1.72040128e-01],\n",
      "        [-2.91105161e-01, -1.81877837e-01],\n",
      "        [-2.77250014e-01, -1.40221645e-01]],\n",
      "\n",
      "       [[-7.34469860e-01,  2.90623108e-01],\n",
      "        [-7.20614713e-01,  2.74585412e-01],\n",
      "        [-7.27542286e-01,  2.81782504e-01],\n",
      "        [-7.20614713e-01,  2.81544933e-01],\n",
      "        [-7.13687139e-01,  2.80109120e-01],\n",
      "        [-7.20614713e-01,  2.81544933e-01],\n",
      "        [-7.13687139e-01,  2.79322767e-01],\n",
      "        [-7.27542286e-01,  2.91696214e-01],\n",
      "        [-7.27542286e-01,  2.87607486e-01],\n",
      "        [-7.34469860e-01,  3.02120817e-01],\n",
      "        [-7.27542286e-01,  2.84388417e-01],\n",
      "        [-7.27542286e-01,  2.84388417e-01],\n",
      "        [-7.27542286e-01,  2.83673069e-01],\n",
      "        [-7.20614713e-01,  2.74425064e-01],\n",
      "        [-7.06759566e-01,  2.61481817e-01],\n",
      "        [-7.20614713e-01,  2.64038727e-01],\n",
      "        [-7.13687139e-01,  2.61436731e-01],\n",
      "        [-7.13687139e-01,  2.76775582e-01],\n",
      "        [-7.20614713e-01,  2.64038727e-01],\n",
      "        [-7.13687139e-01,  2.62342529e-01],\n",
      "        [-7.06759566e-01,  2.44343912e-01],\n",
      "        [-6.99831992e-01,  2.22755292e-01],\n",
      "        [-7.06759566e-01,  2.58671519e-01],\n",
      "        [-7.06759566e-01,  2.37759365e-01],\n",
      "        [-7.13687139e-01,  2.51955708e-01],\n",
      "        [-7.13687139e-01,  2.51955708e-01],\n",
      "        [-6.92904419e-01,  2.34566753e-01],\n",
      "        [-7.06759566e-01,  2.49943873e-01],\n",
      "        [-6.99831992e-01,  2.47982971e-01],\n",
      "        [-6.99831992e-01,  2.47974239e-01]],\n",
      "\n",
      "       [[-6.92904419e-01,  2.34566753e-01],\n",
      "        [-7.06759566e-01,  2.49943873e-01],\n",
      "        [-6.99831992e-01,  2.47982971e-01],\n",
      "        [-6.99831992e-01,  2.47974239e-01],\n",
      "        [-6.99831992e-01,  2.36332038e-01],\n",
      "        [-6.85976846e-01,  2.21944506e-01],\n",
      "        [-6.99831992e-01,  2.36040983e-01],\n",
      "        [-6.85976846e-01,  2.61459944e-01],\n",
      "        [-6.92904419e-01,  2.35851507e-01],\n",
      "        [-6.85976846e-01,  3.86629334e-02],\n",
      "        [-6.65194125e-01,  2.74120768e-02],\n",
      "        [-6.65194125e-01,  1.32629568e-02],\n",
      "        [-6.65194125e-01,  1.32629568e-02],\n",
      "        [-6.58266552e-01,  6.55238840e-03],\n",
      "        [-6.51338979e-01, -1.37459137e-03],\n",
      "        [-6.65194125e-01,  1.17200303e-02],\n",
      "        [-6.65194125e-01,  1.99319397e-02],\n",
      "        [-6.44411405e-01, -6.98651061e-03],\n",
      "        [-6.51338979e-01, -9.68197336e-04],\n",
      "        [-6.51338979e-01, -7.19422517e-03],\n",
      "        [-6.51338979e-01, -9.59159346e-03],\n",
      "        [-6.51338979e-01,  7.79601075e-03],\n",
      "        [-6.16701112e-01, -8.41304489e-03],\n",
      "        [-6.23628685e-01,  2.19823006e-03],\n",
      "        [-6.30556258e-01,  1.06358122e-02],\n",
      "        [-6.16701112e-01, -2.10711769e-02],\n",
      "        [-6.02845965e-01, -4.93384302e-02],\n",
      "        [-6.30556258e-01, -5.04567342e-02],\n",
      "        [-6.16701112e-01, -6.57961835e-02],\n",
      "        [-6.23628685e-01, -5.86336485e-02]],\n",
      "\n",
      "       [[-8.10673167e-01,  5.37005279e-02],\n",
      "        [-8.17600740e-01,  9.89587645e-02],\n",
      "        [-8.17600740e-01,  6.80426032e-02],\n",
      "        [-8.17600740e-01,  6.80426032e-02],\n",
      "        [-8.10673167e-01,  7.33373324e-02],\n",
      "        [-8.03745594e-01,  9.15344865e-02],\n",
      "        [-8.10673167e-01,  9.46542327e-02],\n",
      "        [-8.03745594e-01,  9.33320392e-02],\n",
      "        [-8.03745594e-01,  8.99668672e-02],\n",
      "        [-7.76035300e-01,  6.77978717e-02],\n",
      "        [-7.76035300e-01,  6.47557618e-02],\n",
      "        [-7.69107727e-01,  8.85111426e-02],\n",
      "        [-7.62180153e-01,  9.02696813e-02],\n",
      "        [-7.55252580e-01,  8.11189155e-02],\n",
      "        [-7.55252580e-01,  8.60090497e-02],\n",
      "        [-7.34469860e-01,  2.90623108e-01],\n",
      "        [-7.20614713e-01,  2.74585412e-01],\n",
      "        [-7.27542286e-01,  2.81782504e-01],\n",
      "        [-7.20614713e-01,  2.81544933e-01],\n",
      "        [-7.13687139e-01,  2.80109120e-01],\n",
      "        [-7.20614713e-01,  2.81544933e-01],\n",
      "        [-7.13687139e-01,  2.79322767e-01],\n",
      "        [-7.27542286e-01,  2.91696214e-01],\n",
      "        [-7.27542286e-01,  2.87607486e-01],\n",
      "        [-7.34469860e-01,  3.02120817e-01],\n",
      "        [-7.27542286e-01,  2.84388417e-01],\n",
      "        [-7.27542286e-01,  2.84388417e-01],\n",
      "        [-7.27542286e-01,  2.83673069e-01],\n",
      "        [-7.20614713e-01,  2.74425064e-01],\n",
      "        [-7.06759566e-01,  2.61481817e-01]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[-0.82504617],\n",
      "       [ 0.22502164],\n",
      "       [-0.10699786],\n",
      "       [-0.04242728],\n",
      "       [-0.1485239 ],\n",
      "       [ 0.23633204],\n",
      "       [-0.01841958],\n",
      "       [ 0.26403873]])>, <tf.Tensor: shape=(8,), dtype=float64, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.])>), <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[  2.],\n",
      "       [ 62.],\n",
      "       [104.],\n",
      "       [114.],\n",
      "       [144.],\n",
      "       [ 79.],\n",
      "       [ 92.],\n",
      "       [ 76.]])>)\n",
      "((<tf.Tensor: shape=(8, 30, 2), dtype=float64, numpy=\n",
      "array([[[-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.22632757e+00, -8.73702644e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.21247242e+00,  5.24747575e-01],\n",
      "        [-1.21247242e+00,  1.63292170e-01],\n",
      "        [-1.21247242e+00,  1.75708577e-01],\n",
      "        [-1.19861728e+00, -4.12303617e-02],\n",
      "        [-1.19861728e+00,  1.78301350e-01],\n",
      "        [-1.19168970e+00,  4.23969866e-02],\n",
      "        [-1.19861728e+00,  1.78301350e-01],\n",
      "        [-1.19168970e+00,  1.91278905e-01]],\n",
      "\n",
      "       [[-7.06759566e-01,  2.61481817e-01],\n",
      "        [-7.20614713e-01,  2.64038727e-01],\n",
      "        [-7.13687139e-01,  2.61436731e-01],\n",
      "        [-7.13687139e-01,  2.76775582e-01],\n",
      "        [-7.20614713e-01,  2.64038727e-01],\n",
      "        [-7.13687139e-01,  2.62342529e-01],\n",
      "        [-7.06759566e-01,  2.44343912e-01],\n",
      "        [-6.99831992e-01,  2.22755292e-01],\n",
      "        [-7.06759566e-01,  2.58671519e-01],\n",
      "        [-7.06759566e-01,  2.37759365e-01],\n",
      "        [-7.13687139e-01,  2.51955708e-01],\n",
      "        [-7.13687139e-01,  2.51955708e-01],\n",
      "        [-6.92904419e-01,  2.34566753e-01],\n",
      "        [-7.06759566e-01,  2.49943873e-01],\n",
      "        [-6.99831992e-01,  2.47982971e-01],\n",
      "        [-6.99831992e-01,  2.47974239e-01],\n",
      "        [-6.99831992e-01,  2.36332038e-01],\n",
      "        [-6.85976846e-01,  2.21944506e-01],\n",
      "        [-6.99831992e-01,  2.36040983e-01],\n",
      "        [-6.85976846e-01,  2.61459944e-01],\n",
      "        [-6.92904419e-01,  2.35851507e-01],\n",
      "        [-6.85976846e-01,  3.86629334e-02],\n",
      "        [-6.65194125e-01,  2.74120768e-02],\n",
      "        [-6.65194125e-01,  1.32629568e-02],\n",
      "        [-6.65194125e-01,  1.32629568e-02],\n",
      "        [-6.58266552e-01,  6.55238840e-03],\n",
      "        [-6.51338979e-01, -1.37459137e-03],\n",
      "        [-6.65194125e-01,  1.17200303e-02],\n",
      "        [-6.65194125e-01,  1.99319397e-02],\n",
      "        [-6.44411405e-01, -6.98651061e-03]],\n",
      "\n",
      "       [[-1.02542794e+00,  3.49774916e-01],\n",
      "        [-1.02542794e+00,  3.53465827e-01],\n",
      "        [-1.02542794e+00,  3.89392931e-01],\n",
      "        [-1.02542794e+00,  3.59693191e-01],\n",
      "        [-1.02542794e+00,  4.18158565e-01],\n",
      "        [-1.02542794e+00,  4.18158565e-01],\n",
      "        [-1.01850037e+00,  3.92108947e-01],\n",
      "        [-1.01850037e+00,  3.89786387e-01],\n",
      "        [-1.01157280e+00,  3.51746539e-01],\n",
      "        [-1.01850037e+00,  3.89786387e-01],\n",
      "        [-1.02542794e+00,  3.82013503e-01],\n",
      "        [-1.00464522e+00,  3.26271199e-01],\n",
      "        [-1.00464522e+00,  2.88351886e-01],\n",
      "        [-9.97717649e-01,  5.13696718e-01],\n",
      "        [-1.01157280e+00,  3.24064353e-01],\n",
      "        [-9.97717649e-01,  2.82671070e-01],\n",
      "        [-9.97717649e-01,  2.72006841e-01],\n",
      "        [-1.00464522e+00,  3.05550909e-01],\n",
      "        [-9.76934929e-01,  2.24003644e-01],\n",
      "        [-9.90790076e-01,  3.13763529e-01],\n",
      "        [-9.90790076e-01,  3.13763529e-01],\n",
      "        [-9.90790076e-01,  3.13763529e-01],\n",
      "        [-9.76934929e-01,  3.01499083e-01],\n",
      "        [-9.76934929e-01,  2.74054460e-01],\n",
      "        [-9.63079782e-01,  2.72410367e-01],\n",
      "        [-9.63079782e-01,  2.51660274e-01],\n",
      "        [-9.70007356e-01,  2.68124584e-01],\n",
      "        [-9.56152209e-01,  2.39885197e-01],\n",
      "        [-9.76934929e-01,  2.30013358e-01],\n",
      "        [-9.70007356e-01,  2.24149809e-01]],\n",
      "\n",
      "       [[-9.63079782e-01,  2.10339548e-01],\n",
      "        [-9.56152209e-01,  2.00205250e-01],\n",
      "        [-9.56152209e-01,  2.09419013e-01],\n",
      "        [-9.56152209e-01,  2.02119538e-01],\n",
      "        [-9.42297062e-01,  1.77571735e-01],\n",
      "        [-9.35369488e-01,  3.00116207e-01],\n",
      "        [-9.21514342e-01,  2.55498424e-01],\n",
      "        [-9.28441915e-01,  3.17153314e-01],\n",
      "        [-9.14586768e-01,  2.72756786e-01],\n",
      "        [-9.21514342e-01,  2.66752118e-01],\n",
      "        [-9.00731621e-01,  2.52391387e-01],\n",
      "        [-9.28441915e-01,  2.87495229e-01],\n",
      "        [-9.28441915e-01,  2.87495229e-01],\n",
      "        [-9.28441915e-01,  2.87495229e-01],\n",
      "        [-9.07659195e-01,  3.16682192e-01],\n",
      "        [-9.21514342e-01,  2.62771497e-01],\n",
      "        [-9.14586768e-01,  2.37714287e-01],\n",
      "        [-9.07659195e-01,  2.40904255e-01],\n",
      "        [-9.07659195e-01,  2.14311746e-01],\n",
      "        [-8.93804048e-01,  1.85905901e-01],\n",
      "        [-8.79948901e-01,  2.34237779e-01],\n",
      "        [-8.86876475e-01,  2.43826071e-01],\n",
      "        [-8.79948901e-01,  2.77158692e-01],\n",
      "        [-8.73021328e-01,  2.66406825e-01],\n",
      "        [-8.73021328e-01,  1.53552075e-01],\n",
      "        [-8.79948901e-01,  1.58993137e-01],\n",
      "        [-8.86876475e-01,  1.45483823e-01],\n",
      "        [-8.86876475e-01,  1.46589272e-01],\n",
      "        [-8.79948901e-01,  1.56955551e-01],\n",
      "        [-8.79948901e-01,  1.47698921e-01]],\n",
      "\n",
      "       [[-8.59166181e-01,  1.22230433e-01],\n",
      "        [-8.59166181e-01,  1.22230433e-01],\n",
      "        [-8.52238608e-01,  1.19427050e-01],\n",
      "        [-8.52238608e-01,  1.15569278e-01],\n",
      "        [-8.38383461e-01,  2.33737028e-01],\n",
      "        [-8.45311034e-01,  2.53968149e-01],\n",
      "        [-8.38383461e-01,  2.34381361e-01],\n",
      "        [-8.17600740e-01,  2.20718423e-01],\n",
      "        [-8.17600740e-01,  1.95785477e-01],\n",
      "        [-8.17600740e-01,  2.25021641e-01],\n",
      "        [-8.17600740e-01,  2.22701290e-01],\n",
      "        [-8.17600740e-01,  2.02056727e-01],\n",
      "        [-7.96818020e-01,  2.49215358e-01],\n",
      "        [-8.10673167e-01,  5.37005279e-02],\n",
      "        [-8.17600740e-01,  9.89587645e-02],\n",
      "        [-8.17600740e-01,  6.80426032e-02],\n",
      "        [-8.17600740e-01,  6.80426032e-02],\n",
      "        [-8.10673167e-01,  7.33373324e-02],\n",
      "        [-8.03745594e-01,  9.15344865e-02],\n",
      "        [-8.10673167e-01,  9.46542327e-02],\n",
      "        [-8.03745594e-01,  9.33320392e-02],\n",
      "        [-8.03745594e-01,  8.99668672e-02],\n",
      "        [-7.76035300e-01,  6.77978717e-02],\n",
      "        [-7.76035300e-01,  6.47557618e-02],\n",
      "        [-7.69107727e-01,  8.85111426e-02],\n",
      "        [-7.62180153e-01,  9.02696813e-02],\n",
      "        [-7.55252580e-01,  8.11189155e-02],\n",
      "        [-7.55252580e-01,  8.60090497e-02],\n",
      "        [-7.34469860e-01,  2.90623108e-01],\n",
      "        [-7.20614713e-01,  2.74585412e-01]],\n",
      "\n",
      "       [[-6.51338979e-01, -7.19422517e-03],\n",
      "        [-6.51338979e-01, -9.59159346e-03],\n",
      "        [-6.51338979e-01,  7.79601075e-03],\n",
      "        [-6.16701112e-01, -8.41304489e-03],\n",
      "        [-6.23628685e-01,  2.19823006e-03],\n",
      "        [-6.30556258e-01,  1.06358122e-02],\n",
      "        [-6.16701112e-01, -2.10711769e-02],\n",
      "        [-6.02845965e-01, -4.93384302e-02],\n",
      "        [-6.30556258e-01, -5.04567342e-02],\n",
      "        [-6.16701112e-01, -6.57961835e-02],\n",
      "        [-6.23628685e-01, -5.86336485e-02],\n",
      "        [-6.09773538e-01, -1.84195815e-02],\n",
      "        [-6.30556258e-01, -5.04567342e-02],\n",
      "        [-6.23628685e-01, -5.88116340e-02],\n",
      "        [-6.02845965e-01, -1.10911824e-02],\n",
      "        [-6.30556258e-01, -7.95644161e-02],\n",
      "        [-6.16701112e-01, -9.54971860e-02],\n",
      "        [-6.16701112e-01, -9.46549389e-02],\n",
      "        [-6.02845965e-01, -6.93953862e-02],\n",
      "        [-5.68208098e-01, -1.22637757e-01],\n",
      "        [-5.82063245e-01, -1.17761252e-01],\n",
      "        [-5.75135671e-01, -1.21414606e-01],\n",
      "        [-5.54352951e-01, -1.40310443e-01],\n",
      "        [-5.54352951e-01, -1.41383466e-01],\n",
      "        [-5.68208098e-01, -1.30091043e-01],\n",
      "        [-5.47425377e-01, -1.46470399e-01],\n",
      "        [-5.54352951e-01, -1.52474690e-01],\n",
      "        [-5.54352951e-01, -1.44719034e-01],\n",
      "        [-5.54352951e-01, -1.42419700e-01],\n",
      "        [-5.47425377e-01, -1.49782048e-01]],\n",
      "\n",
      "       [[-1.22632757e+00, -7.69977103e-01],\n",
      "        [-1.22632757e+00, -8.12131572e-01],\n",
      "        [-1.22632757e+00, -4.03360969e-01],\n",
      "        [-1.22632757e+00, -4.87695454e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01],\n",
      "        [-1.23325515e+00, -8.25046168e-01]],\n",
      "\n",
      "       [[-7.13687139e-01,  2.76775582e-01],\n",
      "        [-7.20614713e-01,  2.64038727e-01],\n",
      "        [-7.13687139e-01,  2.62342529e-01],\n",
      "        [-7.06759566e-01,  2.44343912e-01],\n",
      "        [-6.99831992e-01,  2.22755292e-01],\n",
      "        [-7.06759566e-01,  2.58671519e-01],\n",
      "        [-7.06759566e-01,  2.37759365e-01],\n",
      "        [-7.13687139e-01,  2.51955708e-01],\n",
      "        [-7.13687139e-01,  2.51955708e-01],\n",
      "        [-6.92904419e-01,  2.34566753e-01],\n",
      "        [-7.06759566e-01,  2.49943873e-01],\n",
      "        [-6.99831992e-01,  2.47982971e-01],\n",
      "        [-6.99831992e-01,  2.47974239e-01],\n",
      "        [-6.99831992e-01,  2.36332038e-01],\n",
      "        [-6.85976846e-01,  2.21944506e-01],\n",
      "        [-6.99831992e-01,  2.36040983e-01],\n",
      "        [-6.85976846e-01,  2.61459944e-01],\n",
      "        [-6.92904419e-01,  2.35851507e-01],\n",
      "        [-6.85976846e-01,  3.86629334e-02],\n",
      "        [-6.65194125e-01,  2.74120768e-02],\n",
      "        [-6.65194125e-01,  1.32629568e-02],\n",
      "        [-6.65194125e-01,  1.32629568e-02],\n",
      "        [-6.58266552e-01,  6.55238840e-03],\n",
      "        [-6.51338979e-01, -1.37459137e-03],\n",
      "        [-6.65194125e-01,  1.17200303e-02],\n",
      "        [-6.65194125e-01,  1.99319397e-02],\n",
      "        [-6.44411405e-01, -6.98651061e-03],\n",
      "        [-6.51338979e-01, -9.68197336e-04],\n",
      "        [-6.51338979e-01, -7.19422517e-03],\n",
      "        [-6.51338979e-01, -9.59159346e-03]]])>, <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[-0.20866694],\n",
      "       [-0.0009682 ],\n",
      "       [ 0.26754998],\n",
      "       [ 0.14769892],\n",
      "       [ 0.2817825 ],\n",
      "       [-0.14603612],\n",
      "       [-0.82504617],\n",
      "       [ 0.00779601]])>, <tf.Tensor: shape=(8,), dtype=float64, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.])>), <tf.Tensor: shape=(8, 1), dtype=float64, numpy=\n",
      "array([[  9.],\n",
      "       [ 86.],\n",
      "       [ 41.],\n",
      "       [ 53.],\n",
      "       [ 75.],\n",
      "       [102.],\n",
      "       [  2.],\n",
      "       [ 86.]])>)\n"
     ]
    }
   ],
   "source": [
    "show_sample(train, n_sample=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b564f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rnn_unit = 64\n",
    "config.rnn_layer = 3\n",
    "config.fc1_unit = 128\n",
    "config.fc2_unit = 64\n",
    "config.dropout_rate = 0.2\n",
    "config.embedding_dim = 16\n",
    "\n",
    "config.batch_normalization = False\n",
    "config.use_bias = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e68e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input, GRU, LSTM, Concatenate, BatchNormalization, Dropout, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def build_model(rnn_unit, fc1_unit, fc2_unit, embedding_dim, dropout_rate=0, rnn_layer=0, stateful=False, batch_normalization=False):\n",
    "    if stateful:\n",
    "        history_input = Input(batch_shape=(config.batch_size, config.window_size, 2), name='history_input')\n",
    "    else:\n",
    "        history_input = Input(shape=(config.window_size, 2))\n",
    "    price_input = Input(shape=(1), name='price_input')\n",
    "    category_input = Input(shape=(1), name='category_input')\n",
    "    category = Embedding(10, embedding_dim, name='category_embedding')(category_input)\n",
    "    category = Flatten()(category)\n",
    "    \n",
    "#     if config.batch_normalization:\n",
    "#         history_input = BatchNormalization()(history_input)\n",
    "#         price_input = BatchNormalization()(price_input)\n",
    "        \n",
    "    rnn = history_input\n",
    "    for i in range(rnn_layer-1):\n",
    "        rnn = LSTM(rnn_unit, use_bias=config.use_bias, return_sequences=True, dropout=dropout_rate, stateful=stateful)(rnn)\n",
    "    if rnn_layer > 0:\n",
    "        rnn = LSTM(rnn_unit, use_bias=config.use_bias, dropout=dropout_rate, stateful=stateful)(rnn)\n",
    "    else:\n",
    "        rnn = Flatten()(rnn)\n",
    "        \n",
    "    \n",
    "    concat_ = Concatenate()([rnn, price_input, category])\n",
    "    if dropout_rate > 0:\n",
    "        concat_ = Dropout(dropout_rate)(concat_)\n",
    "    fc = Dense(fc1_unit, activation='relu', use_bias=config.use_bias)(concat_)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        fc = Dropout(dropout_rate)(fc)\n",
    "    fc = Dense(fc2_unit, activation='relu', use_bias=config.use_bias)(fc)\n",
    "    \n",
    "    if dropout_rate > 0:\n",
    "        fc = Dropout(dropout_rate)(fc)\n",
    "    output = Dense(1, use_bias=config.use_bias, name='sales_output')(fc)\n",
    "    \n",
    "    model = Model([history_input, price_input, category_input], output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dee151b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    rnn_unit=config.rnn_unit,\n",
    "    rnn_layer=config.rnn_layer,\n",
    "    fc1_unit=config.fc1_unit,\n",
    "    fc2_unit=config.fc2_unit,\n",
    "    stateful=config.stateful,\n",
    "    batch_normalization=config.batch_normalization,\n",
    "    dropout_rate=config.dropout_rate,\n",
    "    embedding_dim=config.embedding_dim,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2a27a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss='mse',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c817454b",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping_cb = EarlyStopping(\n",
    "    monitor=\"loss\",\n",
    "    patience=100,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='../model/demand_forecasting',\n",
    "    monitor=\"val_loss\",\n",
    "    verbose=0,\n",
    "    save_best_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67b201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The save_model argument by default saves the model in the HDF5 format that cannot save custom objects like subclassed models and custom layers. This behavior will be deprecated in a future release in favor of the SavedModel format. Meanwhile, the HDF5 model is saved as W&B files and the SavedModel as W&B Artifacts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "    491/Unknown - 39s 50ms/step - loss: 13567.2617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-14 19:32:34.360684: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 62s 98ms/step - loss: 13567.2617 - val_loss: 38724.4844 - _timestamp: 1655209947.0000 - _runtime: 78.0000\n",
      "Epoch 2/1000\n",
      "491/491 [==============================] - 24s 50ms/step - loss: 6372.0112 - val_loss: 40408.1523 - _timestamp: 1655209990.0000 - _runtime: 121.0000\n",
      "Epoch 3/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 3391.7698"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 42s 86ms/step - loss: 3391.0977 - val_loss: 17819.7754 - _timestamp: 1655210014.0000 - _runtime: 145.0000\n",
      "Epoch 4/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 2842.3406 - val_loss: 30606.2422 - _timestamp: 1655210097.0000 - _runtime: 228.0000\n",
      "Epoch 5/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 3124.3154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 44s 90ms/step - loss: 3123.5593 - val_loss: 16080.6758 - _timestamp: 1655210122.0000 - _runtime: 253.0000\n",
      "Epoch 6/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 2123.9617"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 43s 88ms/step - loss: 2123.4167 - val_loss: 14706.4805 - _timestamp: 1655210166.0000 - _runtime: 297.0000\n",
      "Epoch 7/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 1687.2847"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 42s 86ms/step - loss: 1686.8527 - val_loss: 4057.9768 - _timestamp: 1655210209.0000 - _runtime: 340.0000\n",
      "Epoch 8/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 1255.3363"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 43s 87ms/step - loss: 1255.0562 - val_loss: 3155.7144 - _timestamp: 1655210252.0000 - _runtime: 383.0000\n",
      "Epoch 9/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 1022.1577 - val_loss: 3399.0520 - _timestamp: 1655210334.0000 - _runtime: 465.0000\n",
      "Epoch 10/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 1105.3378 - val_loss: 3993.1274 - _timestamp: 1655210359.0000 - _runtime: 490.0000\n",
      "Epoch 11/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 1727.8340 - val_loss: 4473.2661 - _timestamp: 1655210385.0000 - _runtime: 516.0000\n",
      "Epoch 12/1000\n",
      "491/491 [==============================] - 26s 52ms/step - loss: 1178.6001 - val_loss: 5041.6938 - _timestamp: 1655210410.0000 - _runtime: 541.0000\n",
      "Epoch 13/1000\n",
      "491/491 [==============================] - 26s 52ms/step - loss: 1147.2046 - val_loss: 3876.3438 - _timestamp: 1655210436.0000 - _runtime: 567.0000\n",
      "Epoch 14/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 886.4146 - val_loss: 3339.1169 - _timestamp: 1655210462.0000 - _runtime: 593.0000\n",
      "Epoch 15/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 928.8513"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 32s 65ms/step - loss: 928.8513 - val_loss: 2225.5906 - _timestamp: 1655210481.0000 - _runtime: 612.0000\n",
      "Epoch 16/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 814.0928 - val_loss: 5014.6406 - _timestamp: 1655210511.0000 - _runtime: 642.0000\n",
      "Epoch 17/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 1162.4261 - val_loss: 16603.3281 - _timestamp: 1655210529.0000 - _runtime: 660.0000\n",
      "Epoch 18/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 1347.0854 - val_loss: 3786.4807 - _timestamp: 1655210546.0000 - _runtime: 677.0000\n",
      "Epoch 19/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 1000.8961 - val_loss: 3671.7739 - _timestamp: 1655210564.0000 - _runtime: 695.0000\n",
      "Epoch 20/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 1011.7162 - val_loss: 2707.0764 - _timestamp: 1655210582.0000 - _runtime: 713.0000\n",
      "Epoch 21/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 886.5537 - val_loss: 3643.9990 - _timestamp: 1655210599.0000 - _runtime: 730.0000\n",
      "Epoch 22/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 1005.6291 - val_loss: 2237.8616 - _timestamp: 1655210617.0000 - _runtime: 748.0000\n",
      "Epoch 23/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 995.9565 - val_loss: 2675.2292 - _timestamp: 1655210634.0000 - _runtime: 765.0000\n",
      "Epoch 24/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 887.6645"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 33s 67ms/step - loss: 887.4482 - val_loss: 1565.0867 - _timestamp: 1655210653.0000 - _runtime: 784.0000\n",
      "Epoch 25/1000\n",
      "491/491 [==============================] - 20s 41ms/step - loss: 873.5559 - val_loss: 1888.0504 - _timestamp: 1655210688.0000 - _runtime: 819.0000\n",
      "Epoch 26/1000\n",
      "491/491 [==============================] - 20s 42ms/step - loss: 935.6858 - val_loss: 4699.2666 - _timestamp: 1655210708.0000 - _runtime: 839.0000\n",
      "Epoch 27/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 910.4094 - val_loss: 2801.3591 - _timestamp: 1655210728.0000 - _runtime: 859.0000\n",
      "Epoch 28/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 891.6241 - val_loss: 5810.3950 - _timestamp: 1655210746.0000 - _runtime: 877.0000\n",
      "Epoch 29/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 863.2850 - val_loss: 2928.6667 - _timestamp: 1655210763.0000 - _runtime: 894.0000\n",
      "Epoch 30/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 789.0414"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 31s 62ms/step - loss: 789.0414 - val_loss: 1014.0894 - _timestamp: 1655210781.0000 - _runtime: 912.0000\n",
      "Epoch 31/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 841.7188 - val_loss: 2909.3867 - _timestamp: 1655210816.0000 - _runtime: 947.0000\n",
      "Epoch 32/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 827.5674 - val_loss: 2960.7439 - _timestamp: 1655210842.0000 - _runtime: 973.0000\n",
      "Epoch 33/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 821.0012"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 43s 88ms/step - loss: 821.1224 - val_loss: 907.9177 - _timestamp: 1655210867.0000 - _runtime: 998.0000\n",
      "Epoch 34/1000\n",
      "491/491 [==============================] - 25s 52ms/step - loss: 773.4877 - val_loss: 1281.4504 - _timestamp: 1655210911.0000 - _runtime: 1042.0000\n",
      "Epoch 35/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 761.2560 - val_loss: 1863.6331 - _timestamp: 1655210936.0000 - _runtime: 1067.0000\n",
      "Epoch 36/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 757.0378"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 44s 89ms/step - loss: 757.0378 - val_loss: 753.7808 - _timestamp: 1655210962.0000 - _runtime: 1093.0000\n",
      "Epoch 37/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 745.8665 - val_loss: 2292.1428 - _timestamp: 1655211005.0000 - _runtime: 1136.0000\n",
      "Epoch 38/1000\n",
      "491/491 [==============================] - 27s 56ms/step - loss: 732.8090 - val_loss: 3484.5820 - _timestamp: 1655211033.0000 - _runtime: 1164.0000\n",
      "Epoch 39/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 735.1438 - val_loss: 1100.9999 - _timestamp: 1655211062.0000 - _runtime: 1193.0000\n",
      "Epoch 40/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 749.7448 - val_loss: 3142.8997 - _timestamp: 1655211092.0000 - _runtime: 1223.0000\n",
      "Epoch 41/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 669.1847 - val_loss: 1551.7449 - _timestamp: 1655211124.0000 - _runtime: 1255.0000\n",
      "Epoch 42/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 695.0470 - val_loss: 1382.4327 - _timestamp: 1655211155.0000 - _runtime: 1286.0000\n",
      "Epoch 43/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 687.4745 - val_loss: 847.4086 - _timestamp: 1655211185.0000 - _runtime: 1316.0000\n",
      "Epoch 44/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 643.5065 - val_loss: 978.1143 - _timestamp: 1655211213.0000 - _runtime: 1344.0000\n",
      "Epoch 45/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 659.7404 - val_loss: 832.0418 - _timestamp: 1655211243.0000 - _runtime: 1374.0000\n",
      "Epoch 46/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 621.2331 - val_loss: 1807.5665 - _timestamp: 1655211284.0000 - _runtime: 1415.0000\n",
      "Epoch 47/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 645.3713 - val_loss: 1267.9988 - _timestamp: 1655211313.0000 - _runtime: 1444.0000\n",
      "Epoch 48/1000\n",
      "491/491 [==============================] - 33s 67ms/step - loss: 580.5197 - val_loss: 1908.9181 - _timestamp: 1655211346.0000 - _runtime: 1477.0000\n",
      "Epoch 49/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 637.6282 - val_loss: 1394.3969 - _timestamp: 1655211377.0000 - _runtime: 1508.0000\n",
      "Epoch 50/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 622.0708"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 55s 112ms/step - loss: 622.0708 - val_loss: 497.4197 - _timestamp: 1655211417.0000 - _runtime: 1548.0000\n",
      "Epoch 51/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 613.1020 - val_loss: 1040.7880 - _timestamp: 1655211472.0000 - _runtime: 1603.0000\n",
      "Epoch 52/1000\n",
      "491/491 [==============================] - 28s 58ms/step - loss: 586.2924 - val_loss: 1891.4354 - _timestamp: 1655211501.0000 - _runtime: 1632.0000\n",
      "Epoch 53/1000\n",
      "491/491 [==============================] - 29s 58ms/step - loss: 646.9733 - val_loss: 5504.6816 - _timestamp: 1655211530.0000 - _runtime: 1661.0000\n",
      "Epoch 54/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 603.1834 - val_loss: 1175.5773 - _timestamp: 1655211558.0000 - _runtime: 1689.0000\n",
      "Epoch 55/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 611.7590 - val_loss: 2071.1819 - _timestamp: 1655211586.0000 - _runtime: 1717.0000\n",
      "Epoch 56/1000\n",
      "491/491 [==============================] - 29s 58ms/step - loss: 599.6679 - val_loss: 2453.3286 - _timestamp: 1655211615.0000 - _runtime: 1746.0000\n",
      "Epoch 57/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 566.0566 - val_loss: 2515.2319 - _timestamp: 1655211643.0000 - _runtime: 1774.0000\n",
      "Epoch 58/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 579.5032 - val_loss: 2146.8774 - _timestamp: 1655211671.0000 - _runtime: 1802.0000\n",
      "Epoch 59/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 528.0775 - val_loss: 2299.0330 - _timestamp: 1655211699.0000 - _runtime: 1830.0000\n",
      "Epoch 60/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 582.4884 - val_loss: 1529.1306 - _timestamp: 1655211728.0000 - _runtime: 1859.0000\n",
      "Epoch 61/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 538.6743 - val_loss: 1305.8975 - _timestamp: 1655211755.0000 - _runtime: 1886.0000\n",
      "Epoch 62/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 509.8374 - val_loss: 701.8382 - _timestamp: 1655211798.0000 - _runtime: 1929.0000\n",
      "Epoch 63/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 558.3864 - val_loss: 2840.0740 - _timestamp: 1655211828.0000 - _runtime: 1959.0000\n",
      "Epoch 64/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 506.6539 - val_loss: 718.6489 - _timestamp: 1655211859.0000 - _runtime: 1990.0000\n",
      "Epoch 65/1000\n",
      "491/491 [==============================] - 32s 65ms/step - loss: 543.8989 - val_loss: 743.9803 - _timestamp: 1655211892.0000 - _runtime: 2023.0000\n",
      "Epoch 66/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 504.9185 - val_loss: 1320.6451 - _timestamp: 1655211922.0000 - _runtime: 2053.0000\n",
      "Epoch 67/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 500.4580"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 54s 109ms/step - loss: 500.3376 - val_loss: 482.7707 - _timestamp: 1655211963.0000 - _runtime: 2094.0000\n",
      "Epoch 68/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 510.2975"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 45s 92ms/step - loss: 510.2975 - val_loss: 470.7590 - _timestamp: 1655212042.0000 - _runtime: 2173.0000\n",
      "Epoch 69/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 515.8765 - val_loss: 2109.6663 - _timestamp: 1655212091.0000 - _runtime: 2222.0000\n",
      "Epoch 70/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 479.0243 - val_loss: 1376.4810 - _timestamp: 1655212120.0000 - _runtime: 2251.0000\n",
      "Epoch 71/1000\n",
      "491/491 [==============================] - 29s 58ms/step - loss: 508.6837 - val_loss: 492.5862 - _timestamp: 1655212148.0000 - _runtime: 2279.0000\n",
      "Epoch 72/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 531.7366 - val_loss: 662.5748 - _timestamp: 1655212178.0000 - _runtime: 2309.0000\n",
      "Epoch 73/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 534.8226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 49s 99ms/step - loss: 534.6897 - val_loss: 387.0187 - _timestamp: 1655212206.0000 - _runtime: 2337.0000\n",
      "Epoch 74/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 519.5535 - val_loss: 1690.1624 - _timestamp: 1655212257.0000 - _runtime: 2388.0000\n",
      "Epoch 75/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 507.2793 - val_loss: 728.5684 - _timestamp: 1655212299.0000 - _runtime: 2430.0000\n",
      "Epoch 76/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 466.3051 - val_loss: 708.9742 - _timestamp: 1655212339.0000 - _runtime: 2470.0000\n",
      "Epoch 77/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 494.8387 - val_loss: 387.6079 - _timestamp: 1655212380.0000 - _runtime: 2511.0000\n",
      "Epoch 78/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 465.0272 - val_loss: 490.6824 - _timestamp: 1655212411.0000 - _runtime: 2542.0000\n",
      "Epoch 79/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 491.1905 - val_loss: 2721.8618 - _timestamp: 1655212440.0000 - _runtime: 2571.0000\n",
      "Epoch 80/1000\n",
      "491/491 [==============================] - 30s 62ms/step - loss: 482.1417 - val_loss: 534.6491 - _timestamp: 1655212471.0000 - _runtime: 2602.0000\n",
      "Epoch 81/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 518.7319 - val_loss: 517.4109 - _timestamp: 1655212500.0000 - _runtime: 2631.0000\n",
      "Epoch 82/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 445.9316 - val_loss: 425.7127 - _timestamp: 1655212530.0000 - _runtime: 2661.0000\n",
      "Epoch 83/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 464.3937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 51s 104ms/step - loss: 464.4120 - val_loss: 381.2445 - _timestamp: 1655212560.0000 - _runtime: 2691.0000\n",
      "Epoch 84/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 519.0651 - val_loss: 1817.0062 - _timestamp: 1655212610.0000 - _runtime: 2741.0000\n",
      "Epoch 85/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 488.6369"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 49s 99ms/step - loss: 488.5391 - val_loss: 325.6434 - _timestamp: 1655212639.0000 - _runtime: 2770.0000\n",
      "Epoch 86/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 468.2424 - val_loss: 1212.2369 - _timestamp: 1655212688.0000 - _runtime: 2819.0000\n",
      "Epoch 87/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 483.4370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 48s 99ms/step - loss: 483.5262 - val_loss: 244.8645 - _timestamp: 1655212717.0000 - _runtime: 2848.0000\n",
      "Epoch 88/1000\n",
      "491/491 [==============================] - 29s 58ms/step - loss: 490.7865 - val_loss: 753.7529 - _timestamp: 1655212766.0000 - _runtime: 2897.0000\n",
      "Epoch 89/1000\n",
      "491/491 [==============================] - 29s 58ms/step - loss: 466.1111 - val_loss: 2078.6646 - _timestamp: 1655212807.0000 - _runtime: 2938.0000\n",
      "Epoch 90/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 501.4842 - val_loss: 1607.4323 - _timestamp: 1655212837.0000 - _runtime: 2968.0000\n",
      "Epoch 91/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 465.7687 - val_loss: 755.4568 - _timestamp: 1655212868.0000 - _runtime: 2999.0000\n",
      "Epoch 92/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 501.0505 - val_loss: 2644.9827 - _timestamp: 1655212898.0000 - _runtime: 3029.0000\n",
      "Epoch 93/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 444.6588 - val_loss: 1645.5889 - _timestamp: 1655212939.0000 - _runtime: 3070.0000\n",
      "Epoch 94/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 451.9288 - val_loss: 445.6429 - _timestamp: 1655212969.0000 - _runtime: 3100.0000\n",
      "Epoch 95/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 451.7431 - val_loss: 1448.6603 - _timestamp: 1655213011.0000 - _runtime: 3142.0000\n",
      "Epoch 96/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 423.7197 - val_loss: 2294.5554 - _timestamp: 1655213053.0000 - _runtime: 3184.0000\n",
      "Epoch 97/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 445.9395 - val_loss: 1503.9884 - _timestamp: 1655213084.0000 - _runtime: 3215.0000\n",
      "Epoch 98/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 493.9306 - val_loss: 356.5033 - _timestamp: 1655213115.0000 - _runtime: 3246.0000\n",
      "Epoch 99/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 444.5197 - val_loss: 336.4519 - _timestamp: 1655213146.0000 - _runtime: 3277.0000\n",
      "Epoch 100/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 433.6576 - val_loss: 886.1437 - _timestamp: 1655213176.0000 - _runtime: 3307.0000\n",
      "Epoch 101/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 465.5818 - val_loss: 1872.9469 - _timestamp: 1655213207.0000 - _runtime: 3338.0000\n",
      "Epoch 102/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 423.3985 - val_loss: 874.4856 - _timestamp: 1655213247.0000 - _runtime: 3378.0000\n",
      "Epoch 103/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 454.6153 - val_loss: 252.6076 - _timestamp: 1655213278.0000 - _runtime: 3409.0000\n",
      "Epoch 104/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 445.2099 - val_loss: 1565.8271 - _timestamp: 1655213308.0000 - _runtime: 3439.0000\n",
      "Epoch 105/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 428.2598 - val_loss: 1810.7273 - _timestamp: 1655213338.0000 - _runtime: 3469.0000\n",
      "Epoch 106/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 419.9579 - val_loss: 1624.4049 - _timestamp: 1655213367.0000 - _runtime: 3498.0000\n",
      "Epoch 107/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 401.9300 - val_loss: 530.2416 - _timestamp: 1655213396.0000 - _runtime: 3527.0000\n",
      "Epoch 108/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 426.3751"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 56s 113ms/step - loss: 426.2662 - val_loss: 233.3836 - _timestamp: 1655213427.0000 - _runtime: 3558.0000\n",
      "Epoch 109/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 470.2438 - val_loss: 2360.8418 - _timestamp: 1655213508.0000 - _runtime: 3639.0000\n",
      "Epoch 110/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 370.6347 - val_loss: 1807.3198 - _timestamp: 1655213537.0000 - _runtime: 3668.0000\n",
      "Epoch 111/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 402.4142 - val_loss: 1481.2347 - _timestamp: 1655213567.0000 - _runtime: 3698.0000\n",
      "Epoch 112/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 407.0750 - val_loss: 3831.7229 - _timestamp: 1655213608.0000 - _runtime: 3739.0000\n",
      "Epoch 113/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 462.5437 - val_loss: 359.9333 - _timestamp: 1655213639.0000 - _runtime: 3770.0000\n",
      "Epoch 114/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 435.7176 - val_loss: 2479.8604 - _timestamp: 1655213668.0000 - _runtime: 3799.0000\n",
      "Epoch 115/1000\n",
      "491/491 [==============================] - 31s 64ms/step - loss: 404.6349 - val_loss: 711.5306 - _timestamp: 1655213700.0000 - _runtime: 3831.0000\n",
      "Epoch 116/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 393.2490 - val_loss: 1518.0820 - _timestamp: 1655213730.0000 - _runtime: 3861.0000\n",
      "Epoch 117/1000\n",
      "491/491 [==============================] - 32s 64ms/step - loss: 406.1126 - val_loss: 2148.9128 - _timestamp: 1655213761.0000 - _runtime: 3892.0000\n",
      "Epoch 118/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 391.9897 - val_loss: 849.6729 - _timestamp: 1655213801.0000 - _runtime: 3932.0000\n",
      "Epoch 119/1000\n",
      "491/491 [==============================] - 31s 62ms/step - loss: 410.5815 - val_loss: 1351.6787 - _timestamp: 1655213831.0000 - _runtime: 3962.0000\n",
      "Epoch 120/1000\n",
      "491/491 [==============================] - 29s 60ms/step - loss: 408.7631 - val_loss: 746.8922 - _timestamp: 1655213861.0000 - _runtime: 3992.0000\n",
      "Epoch 121/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 379.0414 - val_loss: 1902.6815 - _timestamp: 1655213891.0000 - _runtime: 4022.0000\n",
      "Epoch 122/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 405.2944 - val_loss: 256.3231 - _timestamp: 1655213921.0000 - _runtime: 4052.0000\n",
      "Epoch 123/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 391.7137 - val_loss: 1232.6975 - _timestamp: 1655213962.0000 - _runtime: 4093.0000\n",
      "Epoch 124/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 400.1841 - val_loss: 2087.1772 - _timestamp: 1655214003.0000 - _runtime: 4134.0000\n",
      "Epoch 125/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 413.3559 - val_loss: 723.2147 - _timestamp: 1655214034.0000 - _runtime: 4165.0000\n",
      "Epoch 126/1000\n",
      "490/491 [============================>.] - ETA: 0s - loss: 407.7185"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 54s 111ms/step - loss: 407.6450 - val_loss: 219.5988 - _timestamp: 1655214063.0000 - _runtime: 4194.0000\n",
      "Epoch 127/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 404.8522 - val_loss: 251.1723 - _timestamp: 1655214146.0000 - _runtime: 4277.0000\n",
      "Epoch 128/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 398.7227 - val_loss: 1289.3167 - _timestamp: 1655214176.0000 - _runtime: 4307.0000\n",
      "Epoch 129/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 403.8056 - val_loss: 350.5474 - _timestamp: 1655214206.0000 - _runtime: 4337.0000\n",
      "Epoch 130/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 406.6136 - val_loss: 253.3416 - _timestamp: 1655214247.0000 - _runtime: 4378.0000\n",
      "Epoch 131/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 406.5372 - val_loss: 823.6451 - _timestamp: 1655214277.0000 - _runtime: 4408.0000\n",
      "Epoch 132/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 419.1954 - val_loss: 3320.7900 - _timestamp: 1655214306.0000 - _runtime: 4437.0000\n",
      "Epoch 133/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 388.0762 - val_loss: 2215.7249 - _timestamp: 1655214337.0000 - _runtime: 4468.0000\n",
      "Epoch 134/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 398.0820 - val_loss: 1465.4744 - _timestamp: 1655214378.0000 - _runtime: 4509.0000\n",
      "Epoch 135/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 381.6254 - val_loss: 922.3514 - _timestamp: 1655214419.0000 - _runtime: 4550.0000\n",
      "Epoch 136/1000\n",
      "491/491 [==============================] - 30s 62ms/step - loss: 386.7545 - val_loss: 846.6510 - _timestamp: 1655214449.0000 - _runtime: 4580.0000\n",
      "Epoch 137/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 368.4835 - val_loss: 1473.9852 - _timestamp: 1655214479.0000 - _runtime: 4610.0000\n",
      "Epoch 138/1000\n",
      "491/491 [==============================] - 30s 62ms/step - loss: 391.1723 - val_loss: 251.6001 - _timestamp: 1655214510.0000 - _runtime: 4641.0000\n",
      "Epoch 139/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 377.3103 - val_loss: 1178.8054 - _timestamp: 1655214540.0000 - _runtime: 4671.0000\n",
      "Epoch 140/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 381.5284 - val_loss: 548.6747 - _timestamp: 1655214570.0000 - _runtime: 4701.0000\n",
      "Epoch 141/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 354.4177 - val_loss: 429.4262 - _timestamp: 1655214601.0000 - _runtime: 4732.0000\n",
      "Epoch 142/1000\n",
      "491/491 [==============================] - 31s 63ms/step - loss: 351.4540 - val_loss: 1859.6331 - _timestamp: 1655214632.0000 - _runtime: 4763.0000\n",
      "Epoch 143/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 361.9605 - val_loss: 279.3393 - _timestamp: 1655214662.0000 - _runtime: 4793.0000\n",
      "Epoch 144/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 370.2496 - val_loss: 566.9537 - _timestamp: 1655214692.0000 - _runtime: 4823.0000\n",
      "Epoch 145/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 390.5006 - val_loss: 1074.5792 - _timestamp: 1655214722.0000 - _runtime: 4853.0000\n",
      "Epoch 146/1000\n",
      "491/491 [==============================] - 30s 61ms/step - loss: 369.5821 - val_loss: 261.6046 - _timestamp: 1655214752.0000 - _runtime: 4883.0000\n",
      "Epoch 147/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 359.6856 - val_loss: 561.9556 - _timestamp: 1655214780.0000 - _runtime: 4911.0000\n",
      "Epoch 148/1000\n",
      "491/491 [==============================] - 27s 56ms/step - loss: 380.7716 - val_loss: 463.7626 - _timestamp: 1655214808.0000 - _runtime: 4939.0000\n",
      "Epoch 149/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 379.4157 - val_loss: 583.1492 - _timestamp: 1655214849.0000 - _runtime: 4980.0000\n",
      "Epoch 150/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 390.4393 - val_loss: 284.0283 - _timestamp: 1655214879.0000 - _runtime: 5010.0000\n",
      "Epoch 151/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 359.5063 - val_loss: 2010.4543 - _timestamp: 1655214906.0000 - _runtime: 5037.0000\n",
      "Epoch 152/1000\n",
      "491/491 [==============================] - 30s 60ms/step - loss: 376.2516 - val_loss: 363.9976 - _timestamp: 1655214936.0000 - _runtime: 5067.0000\n",
      "Epoch 153/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 371.9472 - val_loss: 445.3165 - _timestamp: 1655214965.0000 - _runtime: 5096.0000\n",
      "Epoch 154/1000\n",
      "491/491 [==============================] - 28s 58ms/step - loss: 374.6442 - val_loss: 943.9250 - _timestamp: 1655215006.0000 - _runtime: 5137.0000\n",
      "Epoch 155/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 365.4184 - val_loss: 1220.2556 - _timestamp: 1655215043.0000 - _runtime: 5174.0000\n",
      "Epoch 156/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 358.5258 - val_loss: 1342.3684 - _timestamp: 1655215068.0000 - _runtime: 5199.0000\n",
      "Epoch 157/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 368.2659 - val_loss: 372.8675 - _timestamp: 1655215093.0000 - _runtime: 5224.0000\n",
      "Epoch 158/1000\n",
      "491/491 [==============================] - 24s 50ms/step - loss: 382.4737 - val_loss: 5290.3403 - _timestamp: 1655215118.0000 - _runtime: 5249.0000\n",
      "Epoch 159/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 366.1304 - val_loss: 823.9959 - _timestamp: 1655215143.0000 - _runtime: 5274.0000\n",
      "Epoch 160/1000\n",
      "491/491 [==============================] - 25s 51ms/step - loss: 349.6258 - val_loss: 1700.8378 - _timestamp: 1655215168.0000 - _runtime: 5299.0000\n",
      "Epoch 161/1000\n",
      "491/491 [==============================] - 23s 46ms/step - loss: 352.2095 - val_loss: 608.9908 - _timestamp: 1655215191.0000 - _runtime: 5322.0000\n",
      "Epoch 162/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 375.6077 - val_loss: 2275.2180 - _timestamp: 1655215219.0000 - _runtime: 5350.0000\n",
      "Epoch 163/1000\n",
      "491/491 [==============================] - 28s 56ms/step - loss: 373.7856 - val_loss: 3116.0928 - _timestamp: 1655215260.0000 - _runtime: 5391.0000\n",
      "Epoch 164/1000\n",
      "491/491 [==============================] - 24s 48ms/step - loss: 388.9647 - val_loss: 1195.5853 - _timestamp: 1655215284.0000 - _runtime: 5415.0000\n",
      "Epoch 165/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 318.0446 - val_loss: 742.2031 - _timestamp: 1655215308.0000 - _runtime: 5439.0000\n",
      "Epoch 166/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 377.5168 - val_loss: 1412.1840 - _timestamp: 1655215331.0000 - _runtime: 5462.0000\n",
      "Epoch 167/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 351.1252 - val_loss: 1432.3566 - _timestamp: 1655215353.0000 - _runtime: 5484.0000\n",
      "Epoch 168/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 349.7473 - val_loss: 923.9299 - _timestamp: 1655215375.0000 - _runtime: 5506.0000\n",
      "Epoch 169/1000\n",
      "491/491 [==============================] - 22s 46ms/step - loss: 325.9401 - val_loss: 278.6469 - _timestamp: 1655215398.0000 - _runtime: 5529.0000\n",
      "Epoch 170/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 362.6861 - val_loss: 2344.5361 - _timestamp: 1655215420.0000 - _runtime: 5551.0000\n",
      "Epoch 171/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 371.0363 - val_loss: 267.3938 - _timestamp: 1655215442.0000 - _runtime: 5573.0000\n",
      "Epoch 172/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 372.7816 - val_loss: 2110.6560 - _timestamp: 1655215483.0000 - _runtime: 5614.0000\n",
      "Epoch 173/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 358.1336 - val_loss: 1000.7579 - _timestamp: 1655215505.0000 - _runtime: 5636.0000\n",
      "Epoch 174/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 356.7599 - val_loss: 2818.3784 - _timestamp: 1655215528.0000 - _runtime: 5659.0000\n",
      "Epoch 175/1000\n",
      "491/491 [==============================] - 22s 46ms/step - loss: 354.0920 - val_loss: 1508.6581 - _timestamp: 1655215550.0000 - _runtime: 5681.0000\n",
      "Epoch 176/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 351.1651 - val_loss: 1768.0096 - _timestamp: 1655215573.0000 - _runtime: 5704.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 177/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 340.3421 - val_loss: 2584.5989 - _timestamp: 1655215595.0000 - _runtime: 5726.0000\n",
      "Epoch 178/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 352.7635 - val_loss: 642.3124 - _timestamp: 1655215617.0000 - _runtime: 5748.0000\n",
      "Epoch 179/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 335.7721 - val_loss: 736.4002 - _timestamp: 1655215639.0000 - _runtime: 5770.0000\n",
      "Epoch 180/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 363.5779 - val_loss: 1718.4534 - _timestamp: 1655215662.0000 - _runtime: 5793.0000\n",
      "Epoch 181/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 322.0531 - val_loss: 3660.7622 - _timestamp: 1655215684.0000 - _runtime: 5815.0000\n",
      "Epoch 182/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 346.2340 - val_loss: 1039.4559 - _timestamp: 1655215706.0000 - _runtime: 5837.0000\n",
      "Epoch 183/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 343.2408 - val_loss: 4023.3132 - _timestamp: 1655215747.0000 - _runtime: 5878.0000\n",
      "Epoch 184/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 327.6165 - val_loss: 228.2502 - _timestamp: 1655215769.0000 - _runtime: 5900.0000\n",
      "Epoch 185/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 338.5938 - val_loss: 1270.8457 - _timestamp: 1655215792.0000 - _runtime: 5923.0000\n",
      "Epoch 186/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 369.2896 - val_loss: 419.1223 - _timestamp: 1655215814.0000 - _runtime: 5945.0000\n",
      "Epoch 187/1000\n",
      "491/491 [==============================] - 23s 48ms/step - loss: 344.6473 - val_loss: 723.1967 - _timestamp: 1655215838.0000 - _runtime: 5969.0000\n",
      "Epoch 188/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 299.7552 - val_loss: 1260.1029 - _timestamp: 1655215860.0000 - _runtime: 5991.0000\n",
      "Epoch 189/1000\n",
      "491/491 [==============================] - 23s 46ms/step - loss: 322.3474 - val_loss: 1571.9302 - _timestamp: 1655215883.0000 - _runtime: 6014.0000\n",
      "Epoch 190/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 359.0796 - val_loss: 530.5128 - _timestamp: 1655215905.0000 - _runtime: 6036.0000\n",
      "Epoch 191/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 326.9278 - val_loss: 1155.4048 - _timestamp: 1655215928.0000 - _runtime: 6059.0000\n",
      "Epoch 192/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 337.7489 - val_loss: 1795.4270 - _timestamp: 1655215950.0000 - _runtime: 6081.0000\n",
      "Epoch 193/1000\n",
      "491/491 [==============================] - 23s 46ms/step - loss: 329.6872 - val_loss: 4700.6934 - _timestamp: 1655215973.0000 - _runtime: 6104.0000\n",
      "Epoch 194/1000\n",
      "491/491 [==============================] - 23s 46ms/step - loss: 318.9863 - val_loss: 224.7804 - _timestamp: 1655215996.0000 - _runtime: 6127.0000\n",
      "Epoch 195/1000\n",
      "491/491 [==============================] - 23s 46ms/step - loss: 352.6359 - val_loss: 1502.8676 - _timestamp: 1655216019.0000 - _runtime: 6150.0000\n",
      "Epoch 196/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 309.6007"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 311s 634ms/step - loss: 309.6007 - val_loss: 201.0103 - _timestamp: 1655216042.0000 - _runtime: 6173.0000\n",
      "Epoch 197/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 319.8521 - val_loss: 873.5762 - _timestamp: 1655216352.0000 - _runtime: 6483.0000\n",
      "Epoch 198/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 309.1583 - val_loss: 3222.7593 - _timestamp: 1655216373.0000 - _runtime: 6504.0000\n",
      "Epoch 199/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 327.7256 - val_loss: 1585.1727 - _timestamp: 1655216394.0000 - _runtime: 6525.0000\n",
      "Epoch 200/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 333.4580 - val_loss: 689.4443 - _timestamp: 1655216415.0000 - _runtime: 6546.0000\n",
      "Epoch 201/1000\n",
      "491/491 [==============================] - 21s 44ms/step - loss: 324.1508 - val_loss: 1280.6323 - _timestamp: 1655216437.0000 - _runtime: 6568.0000\n",
      "Epoch 202/1000\n",
      "491/491 [==============================] - 21s 44ms/step - loss: 313.6128 - val_loss: 711.1025 - _timestamp: 1655216459.0000 - _runtime: 6590.0000\n",
      "Epoch 203/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 342.8442 - val_loss: 698.1148 - _timestamp: 1655216480.0000 - _runtime: 6611.0000\n",
      "Epoch 204/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 317.8372 - val_loss: 1089.3566 - _timestamp: 1655216501.0000 - _runtime: 6632.0000\n",
      "Epoch 205/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 339.2491 - val_loss: 266.4998 - _timestamp: 1655216522.0000 - _runtime: 6653.0000\n",
      "Epoch 206/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 330.4351 - val_loss: 725.6122 - _timestamp: 1655216544.0000 - _runtime: 6675.0000\n",
      "Epoch 207/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 268.0951 - val_loss: 1103.3337 - _timestamp: 1655216565.0000 - _runtime: 6696.0000\n",
      "Epoch 208/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 311.9881 - val_loss: 1110.4216 - _timestamp: 1655216586.0000 - _runtime: 6717.0000\n",
      "Epoch 209/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 314.4720 - val_loss: 1175.9407 - _timestamp: 1655216607.0000 - _runtime: 6738.0000\n",
      "Epoch 210/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 301.7206 - val_loss: 453.8138 - _timestamp: 1655216628.0000 - _runtime: 6759.0000\n",
      "Epoch 211/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 301.0137 - val_loss: 524.0048 - _timestamp: 1655216649.0000 - _runtime: 6780.0000\n",
      "Epoch 212/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 297.5029 - val_loss: 1178.0897 - _timestamp: 1655216671.0000 - _runtime: 6802.0000\n",
      "Epoch 213/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 299.1144 - val_loss: 813.7968 - _timestamp: 1655216692.0000 - _runtime: 6823.0000\n",
      "Epoch 214/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 291.7045 - val_loss: 785.9770 - _timestamp: 1655216713.0000 - _runtime: 6844.0000\n",
      "Epoch 215/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 292.4768 - val_loss: 1973.1136 - _timestamp: 1655216735.0000 - _runtime: 6866.0000\n",
      "Epoch 216/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 303.7186 - val_loss: 1631.0511 - _timestamp: 1655216756.0000 - _runtime: 6887.0000\n",
      "Epoch 217/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 300.4994 - val_loss: 1467.5786 - _timestamp: 1655216778.0000 - _runtime: 6909.0000\n",
      "Epoch 218/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 277.9460 - val_loss: 442.1561 - _timestamp: 1655216800.0000 - _runtime: 6931.0000\n",
      "Epoch 219/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 284.0543 - val_loss: 497.5674 - _timestamp: 1655216821.0000 - _runtime: 6952.0000\n",
      "Epoch 220/1000\n",
      "491/491 [==============================] - 20s 41ms/step - loss: 304.2839 - val_loss: 2292.4719 - _timestamp: 1655216842.0000 - _runtime: 6973.0000\n",
      "Epoch 221/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 311.3184 - val_loss: 338.3922 - _timestamp: 1655216860.0000 - _runtime: 6991.0000\n",
      "Epoch 222/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 293.6678 - val_loss: 1301.1388 - _timestamp: 1655216879.0000 - _runtime: 7010.0000\n",
      "Epoch 223/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 290.5197 - val_loss: 334.3929 - _timestamp: 1655216898.0000 - _runtime: 7029.0000\n",
      "Epoch 224/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 295.9783 - val_loss: 1397.0254 - _timestamp: 1655216917.0000 - _runtime: 7048.0000\n",
      "Epoch 225/1000\n",
      "491/491 [==============================] - 20s 40ms/step - loss: 271.6223 - val_loss: 762.0163 - _timestamp: 1655216937.0000 - _runtime: 7068.0000\n",
      "Epoch 226/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 272.9174 - val_loss: 903.7410 - _timestamp: 1655216956.0000 - _runtime: 7087.0000\n",
      "Epoch 227/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 297.4887 - val_loss: 2004.4438 - _timestamp: 1655216977.0000 - _runtime: 7108.0000\n",
      "Epoch 228/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 289.6916 - val_loss: 1028.7771 - _timestamp: 1655217002.0000 - _runtime: 7133.0000\n",
      "Epoch 229/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 268.5659 - val_loss: 1701.6675 - _timestamp: 1655217024.0000 - _runtime: 7155.0000\n",
      "Epoch 230/1000\n",
      "491/491 [==============================] - 44s 90ms/step - loss: 265.1888 - val_loss: 2202.9229 - _timestamp: 1655217065.0000 - _runtime: 7196.0000\n",
      "Epoch 231/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 292.9587 - val_loss: 591.4441 - _timestamp: 1655217090.0000 - _runtime: 7221.0000\n",
      "Epoch 232/1000\n",
      "491/491 [==============================] - 20s 41ms/step - loss: 268.9855 - val_loss: 972.6338 - _timestamp: 1655217110.0000 - _runtime: 7241.0000\n",
      "Epoch 233/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 306.2431 - val_loss: 1252.2909 - _timestamp: 1655217129.0000 - _runtime: 7260.0000\n",
      "Epoch 234/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 283.2339 - val_loss: 204.7569 - _timestamp: 1655217148.0000 - _runtime: 7279.0000\n",
      "Epoch 235/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 285.4431 - val_loss: 1990.4692 - _timestamp: 1655217167.0000 - _runtime: 7298.0000\n",
      "Epoch 236/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 297.9359 - val_loss: 587.2154 - _timestamp: 1655217186.0000 - _runtime: 7317.0000\n",
      "Epoch 237/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 285.7712 - val_loss: 1250.7192 - _timestamp: 1655217206.0000 - _runtime: 7337.0000\n",
      "Epoch 238/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 311.4697 - val_loss: 1339.0319 - _timestamp: 1655217225.0000 - _runtime: 7356.0000\n",
      "Epoch 239/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 293.2437 - val_loss: 2206.2891 - _timestamp: 1655217244.0000 - _runtime: 7375.0000\n",
      "Epoch 240/1000\n",
      "491/491 [==============================] - 20s 39ms/step - loss: 297.7835 - val_loss: 885.8232 - _timestamp: 1655217263.0000 - _runtime: 7394.0000\n",
      "Epoch 241/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 281.3118 - val_loss: 1568.5609 - _timestamp: 1655217283.0000 - _runtime: 7414.0000\n",
      "Epoch 242/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 274.7291 - val_loss: 2083.8557 - _timestamp: 1655217302.0000 - _runtime: 7433.0000\n",
      "Epoch 243/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 274.8737 - val_loss: 835.6976 - _timestamp: 1655217321.0000 - _runtime: 7452.0000\n",
      "Epoch 244/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 275.8325 - val_loss: 439.9576 - _timestamp: 1655217341.0000 - _runtime: 7472.0000\n",
      "Epoch 245/1000\n",
      "491/491 [==============================] - 20s 40ms/step - loss: 294.0927 - val_loss: 467.0939 - _timestamp: 1655217360.0000 - _runtime: 7491.0000\n",
      "Epoch 246/1000\n",
      "491/491 [==============================] - 24s 46ms/step - loss: 271.5610 - val_loss: 1823.0776 - _timestamp: 1655217384.0000 - _runtime: 7515.0000\n",
      "Epoch 247/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 276.9781 - val_loss: 1992.3694 - _timestamp: 1655217403.0000 - _runtime: 7534.0000\n",
      "Epoch 248/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 249.1246 - val_loss: 1735.2242 - _timestamp: 1655217421.0000 - _runtime: 7552.0000\n",
      "Epoch 249/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 276.5250 - val_loss: 826.7741 - _timestamp: 1655217441.0000 - _runtime: 7572.0000\n",
      "Epoch 250/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 293.1136 - val_loss: 1239.2720 - _timestamp: 1655217459.0000 - _runtime: 7590.0000\n",
      "Epoch 251/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 256.7054 - val_loss: 900.2668 - _timestamp: 1655217477.0000 - _runtime: 7608.0000\n",
      "Epoch 252/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 266.7720 - val_loss: 1235.2654 - _timestamp: 1655217496.0000 - _runtime: 7627.0000\n",
      "Epoch 253/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 268.1523 - val_loss: 1853.2848 - _timestamp: 1655217514.0000 - _runtime: 7645.0000\n",
      "Epoch 254/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 260.6792 - val_loss: 948.9193 - _timestamp: 1655217532.0000 - _runtime: 7663.0000\n",
      "Epoch 255/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 263.6243 - val_loss: 1543.1178 - _timestamp: 1655217551.0000 - _runtime: 7682.0000\n",
      "Epoch 256/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 263.4374 - val_loss: 1062.7622 - _timestamp: 1655217569.0000 - _runtime: 7700.0000\n",
      "Epoch 257/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 271.4007 - val_loss: 218.7347 - _timestamp: 1655217587.0000 - _runtime: 7718.0000\n",
      "Epoch 258/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 280.1148 - val_loss: 1051.0304 - _timestamp: 1655217605.0000 - _runtime: 7736.0000\n",
      "Epoch 259/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 267.2255 - val_loss: 859.9393 - _timestamp: 1655217623.0000 - _runtime: 7754.0000\n",
      "Epoch 260/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 268.6373 - val_loss: 1765.9479 - _timestamp: 1655217642.0000 - _runtime: 7773.0000\n",
      "Epoch 261/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 288.4951 - val_loss: 1262.6398 - _timestamp: 1655217660.0000 - _runtime: 7791.0000\n",
      "Epoch 262/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 265.4180 - val_loss: 595.0345 - _timestamp: 1655217679.0000 - _runtime: 7810.0000\n",
      "Epoch 263/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 263.1143 - val_loss: 953.0359 - _timestamp: 1655217697.0000 - _runtime: 7828.0000\n",
      "Epoch 264/1000\n",
      "491/491 [==============================] - 18s 38ms/step - loss: 276.0973 - val_loss: 301.1817 - _timestamp: 1655217716.0000 - _runtime: 7847.0000\n",
      "Epoch 265/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 288.2195 - val_loss: 2295.1052 - _timestamp: 1655217735.0000 - _runtime: 7866.0000\n",
      "Epoch 266/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 268.7652 - val_loss: 517.5891 - _timestamp: 1655217753.0000 - _runtime: 7884.0000\n",
      "Epoch 267/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 279.2387 - val_loss: 691.9868 - _timestamp: 1655217771.0000 - _runtime: 7902.0000\n",
      "Epoch 268/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 260.9395 - val_loss: 600.2498 - _timestamp: 1655217790.0000 - _runtime: 7921.0000\n",
      "Epoch 269/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 292.4051 - val_loss: 1034.3495 - _timestamp: 1655217808.0000 - _runtime: 7939.0000\n",
      "Epoch 270/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 277.1681 - val_loss: 353.7269 - _timestamp: 1655217826.0000 - _runtime: 7957.0000\n",
      "Epoch 271/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 261.9898 - val_loss: 352.2858 - _timestamp: 1655217845.0000 - _runtime: 7976.0000\n",
      "Epoch 272/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 254.7485 - val_loss: 2286.8579 - _timestamp: 1655217863.0000 - _runtime: 7994.0000\n",
      "Epoch 273/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 277.6741 - val_loss: 737.8081 - _timestamp: 1655217881.0000 - _runtime: 8012.0000\n",
      "Epoch 274/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 269.3470 - val_loss: 1954.4235 - _timestamp: 1655217900.0000 - _runtime: 8031.0000\n",
      "Epoch 275/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 266.9291 - val_loss: 1473.1467 - _timestamp: 1655217918.0000 - _runtime: 8049.0000\n",
      "Epoch 276/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 276.4846 - val_loss: 916.2687 - _timestamp: 1655217939.0000 - _runtime: 8070.0000\n",
      "Epoch 277/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 242.1528 - val_loss: 1429.4968 - _timestamp: 1655217957.0000 - _runtime: 8088.0000\n",
      "Epoch 278/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 261.7444 - val_loss: 945.8563 - _timestamp: 1655217975.0000 - _runtime: 8106.0000\n",
      "Epoch 279/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 269.9346 - val_loss: 946.4081 - _timestamp: 1655217994.0000 - _runtime: 8125.0000\n",
      "Epoch 280/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 270.9437 - val_loss: 1079.4342 - _timestamp: 1655218012.0000 - _runtime: 8143.0000\n",
      "Epoch 281/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 242.3720 - val_loss: 840.7411 - _timestamp: 1655218030.0000 - _runtime: 8161.0000\n",
      "Epoch 282/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 250.5266 - val_loss: 267.4452 - _timestamp: 1655218049.0000 - _runtime: 8180.0000\n",
      "Epoch 283/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 262.7675 - val_loss: 897.8826 - _timestamp: 1655218067.0000 - _runtime: 8198.0000\n",
      "Epoch 284/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 270.3745 - val_loss: 432.3076 - _timestamp: 1655218085.0000 - _runtime: 8216.0000\n",
      "Epoch 285/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 250.1991 - val_loss: 215.4001 - _timestamp: 1655218104.0000 - _runtime: 8235.0000\n",
      "Epoch 286/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 262.1682 - val_loss: 940.4064 - _timestamp: 1655218122.0000 - _runtime: 8253.0000\n",
      "Epoch 287/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 260.6203 - val_loss: 1449.2273 - _timestamp: 1655218140.0000 - _runtime: 8271.0000\n",
      "Epoch 288/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 270.3394 - val_loss: 1184.8296 - _timestamp: 1655218159.0000 - _runtime: 8290.0000\n",
      "Epoch 289/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 259.9646 - val_loss: 899.8213 - _timestamp: 1655218177.0000 - _runtime: 8308.0000\n",
      "Epoch 290/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 261.8264 - val_loss: 2870.0520 - _timestamp: 1655218196.0000 - _runtime: 8327.0000\n",
      "Epoch 291/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 272.9592 - val_loss: 1666.9816 - _timestamp: 1655218214.0000 - _runtime: 8345.0000\n",
      "Epoch 292/1000\n",
      "491/491 [==============================] - 20s 40ms/step - loss: 274.6072 - val_loss: 1424.3505 - _timestamp: 1655218234.0000 - _runtime: 8365.0000\n",
      "Epoch 293/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 275.9287 - val_loss: 2249.7942 - _timestamp: 1655218253.0000 - _runtime: 8384.0000\n",
      "Epoch 294/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 271.1209 - val_loss: 1757.1160 - _timestamp: 1655218273.0000 - _runtime: 8404.0000\n",
      "Epoch 295/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 242.8480 - val_loss: 1677.3590 - _timestamp: 1655218292.0000 - _runtime: 8423.0000\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 19s 39ms/step - loss: 267.9102 - val_loss: 777.6199 - _timestamp: 1655218311.0000 - _runtime: 8442.0000\n",
      "Epoch 297/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 253.8812 - val_loss: 764.1968 - _timestamp: 1655218332.0000 - _runtime: 8463.0000\n",
      "Epoch 298/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 265.2142 - val_loss: 1999.0411 - _timestamp: 1655218351.0000 - _runtime: 8482.0000\n",
      "Epoch 299/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 248.6931 - val_loss: 472.9641 - _timestamp: 1655218370.0000 - _runtime: 8501.0000\n",
      "Epoch 300/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 274.4395 - val_loss: 2130.4705 - _timestamp: 1655218390.0000 - _runtime: 8521.0000\n",
      "Epoch 301/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 276.7388 - val_loss: 1573.3269 - _timestamp: 1655218409.0000 - _runtime: 8540.0000\n",
      "Epoch 302/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 282.0948 - val_loss: 786.6022 - _timestamp: 1655218428.0000 - _runtime: 8559.0000\n",
      "Epoch 303/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 257.9362 - val_loss: 1489.5414 - _timestamp: 1655218447.0000 - _runtime: 8578.0000\n",
      "Epoch 304/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 269.6173 - val_loss: 378.4350 - _timestamp: 1655218466.0000 - _runtime: 8597.0000\n",
      "Epoch 305/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 287.2161 - val_loss: 1599.3716 - _timestamp: 1655218486.0000 - _runtime: 8617.0000\n",
      "Epoch 306/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 272.4719 - val_loss: 1949.0667 - _timestamp: 1655218505.0000 - _runtime: 8636.0000\n",
      "Epoch 307/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 255.5223 - val_loss: 233.9528 - _timestamp: 1655218524.0000 - _runtime: 8655.0000\n",
      "Epoch 308/1000\n",
      "491/491 [==============================] - 20s 41ms/step - loss: 283.2951 - val_loss: 1585.9348 - _timestamp: 1655218544.0000 - _runtime: 8675.0000\n",
      "Epoch 309/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 243.3092 - val_loss: 1366.0951 - _timestamp: 1655218564.0000 - _runtime: 8695.0000\n",
      "Epoch 310/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 280.0159 - val_loss: 1420.5841 - _timestamp: 1655218582.0000 - _runtime: 8713.0000\n",
      "Epoch 311/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 258.6340 - val_loss: 705.3888 - _timestamp: 1655218600.0000 - _runtime: 8731.0000\n",
      "Epoch 312/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 247.4800 - val_loss: 448.8872 - _timestamp: 1655218619.0000 - _runtime: 8750.0000\n",
      "Epoch 313/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 251.4838 - val_loss: 1231.6086 - _timestamp: 1655218637.0000 - _runtime: 8768.0000\n",
      "Epoch 314/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 253.2018 - val_loss: 1130.5596 - _timestamp: 1655218655.0000 - _runtime: 8786.0000\n",
      "Epoch 315/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 266.1762 - val_loss: 293.4261 - _timestamp: 1655218673.0000 - _runtime: 8804.0000\n",
      "Epoch 316/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 248.4076 - val_loss: 963.2837 - _timestamp: 1655218692.0000 - _runtime: 8823.0000\n",
      "Epoch 317/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 271.8028 - val_loss: 1369.1963 - _timestamp: 1655218710.0000 - _runtime: 8841.0000\n",
      "Epoch 318/1000\n",
      "491/491 [==============================] - 20s 40ms/step - loss: 260.7109 - val_loss: 1370.7529 - _timestamp: 1655218730.0000 - _runtime: 8861.0000\n",
      "Epoch 319/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 266.2036 - val_loss: 971.3793 - _timestamp: 1655218750.0000 - _runtime: 8881.0000\n",
      "Epoch 320/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 259.8087 - val_loss: 1135.3970 - _timestamp: 1655218769.0000 - _runtime: 8900.0000\n",
      "Epoch 321/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 274.6904 - val_loss: 3497.1536 - _timestamp: 1655218788.0000 - _runtime: 8919.0000\n",
      "Epoch 322/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 260.0242 - val_loss: 204.1736 - _timestamp: 1655218807.0000 - _runtime: 8938.0000\n",
      "Epoch 323/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 252.0391 - val_loss: 604.4620 - _timestamp: 1655218827.0000 - _runtime: 8958.0000\n",
      "Epoch 324/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 273.2345 - val_loss: 369.2877 - _timestamp: 1655218846.0000 - _runtime: 8977.0000\n",
      "Epoch 325/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 232.2785 - val_loss: 1498.5582 - _timestamp: 1655218866.0000 - _runtime: 8997.0000\n",
      "Epoch 326/1000\n",
      "491/491 [==============================] - 20s 40ms/step - loss: 231.4322 - val_loss: 1532.9180 - _timestamp: 1655218886.0000 - _runtime: 9017.0000\n",
      "Epoch 327/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 250.3823 - val_loss: 403.6761 - _timestamp: 1655218905.0000 - _runtime: 9036.0000\n",
      "Epoch 328/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 263.2677 - val_loss: 1532.2579 - _timestamp: 1655218924.0000 - _runtime: 9055.0000\n",
      "Epoch 329/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 270.0432 - val_loss: 476.9873 - _timestamp: 1655218944.0000 - _runtime: 9075.0000\n",
      "Epoch 330/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 248.8798 - val_loss: 240.3697 - _timestamp: 1655218963.0000 - _runtime: 9094.0000\n",
      "Epoch 331/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 270.1599 - val_loss: 1245.8180 - _timestamp: 1655218983.0000 - _runtime: 9114.0000\n",
      "Epoch 332/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 262.4357 - val_loss: 619.4890 - _timestamp: 1655219002.0000 - _runtime: 9133.0000\n",
      "Epoch 333/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 251.2826 - val_loss: 2666.5671 - _timestamp: 1655219021.0000 - _runtime: 9152.0000\n",
      "Epoch 334/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 240.9889 - val_loss: 766.2132 - _timestamp: 1655219040.0000 - _runtime: 9171.0000\n",
      "Epoch 335/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 258.3057 - val_loss: 763.7690 - _timestamp: 1655219060.0000 - _runtime: 9191.0000\n",
      "Epoch 336/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 243.0228 - val_loss: 556.5981 - _timestamp: 1655219079.0000 - _runtime: 9210.0000\n",
      "Epoch 337/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 257.7663 - val_loss: 912.8578 - _timestamp: 1655219098.0000 - _runtime: 9229.0000\n",
      "Epoch 338/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 267.3676 - val_loss: 1090.7920 - _timestamp: 1655219118.0000 - _runtime: 9249.0000\n",
      "Epoch 339/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 227.3791 - val_loss: 310.0438 - _timestamp: 1655219137.0000 - _runtime: 9268.0000\n",
      "Epoch 340/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 283.9403 - val_loss: 419.0975 - _timestamp: 1655219156.0000 - _runtime: 9287.0000\n",
      "Epoch 341/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 264.9265 - val_loss: 1733.0300 - _timestamp: 1655219175.0000 - _runtime: 9306.0000\n",
      "Epoch 342/1000\n",
      "491/491 [==============================] - 19s 40ms/step - loss: 251.5640 - val_loss: 1437.8502 - _timestamp: 1655219195.0000 - _runtime: 9326.0000\n",
      "Epoch 343/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 270.8673 - val_loss: 2301.4639 - _timestamp: 1655219214.0000 - _runtime: 9345.0000\n",
      "Epoch 344/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 265.4652 - val_loss: 1151.5122 - _timestamp: 1655219234.0000 - _runtime: 9365.0000\n",
      "Epoch 345/1000\n",
      "491/491 [==============================] - 19s 40ms/step - loss: 253.0583 - val_loss: 3292.7434 - _timestamp: 1655219253.0000 - _runtime: 9384.0000\n",
      "Epoch 346/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 271.4666 - val_loss: 959.8871 - _timestamp: 1655219273.0000 - _runtime: 9404.0000\n",
      "Epoch 347/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 230.9789 - val_loss: 592.8630 - _timestamp: 1655219292.0000 - _runtime: 9423.0000\n",
      "Epoch 348/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 248.7684 - val_loss: 272.5663 - _timestamp: 1655219312.0000 - _runtime: 9443.0000\n",
      "Epoch 349/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 266.8872 - val_loss: 925.3677 - _timestamp: 1655219331.0000 - _runtime: 9462.0000\n",
      "Epoch 350/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 272.0677 - val_loss: 438.5736 - _timestamp: 1655219351.0000 - _runtime: 9482.0000\n",
      "Epoch 351/1000\n",
      "491/491 [==============================] - 20s 41ms/step - loss: 255.3443 - val_loss: 846.4169 - _timestamp: 1655219371.0000 - _runtime: 9502.0000\n",
      "Epoch 352/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 257.2206 - val_loss: 950.7019 - _timestamp: 1655219395.0000 - _runtime: 9526.0000\n",
      "Epoch 353/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 249.5930 - val_loss: 935.0140 - _timestamp: 1655219420.0000 - _runtime: 9551.0000\n",
      "Epoch 354/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 223.9702 - val_loss: 267.6571 - _timestamp: 1655219445.0000 - _runtime: 9576.0000\n",
      "Epoch 355/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 251.3848 - val_loss: 4993.1812 - _timestamp: 1655219486.0000 - _runtime: 9617.0000\n",
      "Epoch 356/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 269.8960 - val_loss: 821.3580 - _timestamp: 1655219510.0000 - _runtime: 9641.0000\n",
      "Epoch 357/1000\n",
      "491/491 [==============================] - 24s 50ms/step - loss: 272.6653 - val_loss: 1164.5406 - _timestamp: 1655219535.0000 - _runtime: 9666.0000\n",
      "Epoch 358/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 228.9545 - val_loss: 1098.8621 - _timestamp: 1655219560.0000 - _runtime: 9691.0000\n",
      "Epoch 359/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 251.0309 - val_loss: 1765.7721 - _timestamp: 1655219585.0000 - _runtime: 9716.0000\n",
      "Epoch 360/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 247.8170 - val_loss: 366.1360 - _timestamp: 1655219609.0000 - _runtime: 9740.0000\n",
      "Epoch 361/1000\n",
      "491/491 [==============================] - 24s 50ms/step - loss: 251.9256 - val_loss: 303.8986 - _timestamp: 1655219634.0000 - _runtime: 9765.0000\n",
      "Epoch 362/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 264.4983 - val_loss: 769.2978 - _timestamp: 1655219658.0000 - _runtime: 9789.0000\n",
      "Epoch 363/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 248.1777 - val_loss: 515.2783 - _timestamp: 1655219683.0000 - _runtime: 9814.0000\n",
      "Epoch 364/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 226.2455 - val_loss: 469.6809 - _timestamp: 1655219702.0000 - _runtime: 9833.0000\n",
      "Epoch 365/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 235.3564 - val_loss: 547.5866 - _timestamp: 1655219720.0000 - _runtime: 9851.0000\n",
      "Epoch 366/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 252.4452 - val_loss: 298.4025 - _timestamp: 1655219739.0000 - _runtime: 9870.0000\n",
      "Epoch 367/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 240.7414 - val_loss: 918.0157 - _timestamp: 1655219757.0000 - _runtime: 9888.0000\n",
      "Epoch 368/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 247.8290 - val_loss: 2271.2219 - _timestamp: 1655219775.0000 - _runtime: 9906.0000\n",
      "Epoch 369/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 242.5274 - val_loss: 1151.3430 - _timestamp: 1655219793.0000 - _runtime: 9924.0000\n",
      "Epoch 370/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 243.5817 - val_loss: 872.5024 - _timestamp: 1655219811.0000 - _runtime: 9942.0000\n",
      "Epoch 371/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 251.3228 - val_loss: 1122.5106 - _timestamp: 1655219829.0000 - _runtime: 9960.0000\n",
      "Epoch 372/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 245.2787 - val_loss: 1663.1322 - _timestamp: 1655219848.0000 - _runtime: 9979.0000\n",
      "Epoch 373/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 234.4183 - val_loss: 354.5147 - _timestamp: 1655219866.0000 - _runtime: 9997.0000\n",
      "Epoch 374/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 251.6429 - val_loss: 1678.5662 - _timestamp: 1655219884.0000 - _runtime: 10015.0000\n",
      "Epoch 375/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 240.0596 - val_loss: 1012.8631 - _timestamp: 1655219902.0000 - _runtime: 10033.0000\n",
      "Epoch 376/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 240.6907 - val_loss: 2652.8733 - _timestamp: 1655219920.0000 - _runtime: 10051.0000\n",
      "Epoch 377/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 248.7810 - val_loss: 835.2713 - _timestamp: 1655219938.0000 - _runtime: 10069.0000\n",
      "Epoch 378/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 255.6307 - val_loss: 350.8902 - _timestamp: 1655219956.0000 - _runtime: 10087.0000\n",
      "Epoch 379/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 242.7423 - val_loss: 1696.2465 - _timestamp: 1655219974.0000 - _runtime: 10105.0000\n",
      "Epoch 380/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 254.9117 - val_loss: 827.1077 - _timestamp: 1655219992.0000 - _runtime: 10123.0000\n",
      "Epoch 381/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 233.8071 - val_loss: 2001.4722 - _timestamp: 1655220010.0000 - _runtime: 10141.0000\n",
      "Epoch 382/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 255.3274 - val_loss: 601.5225 - _timestamp: 1655220028.0000 - _runtime: 10159.0000\n",
      "Epoch 383/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 243.6869 - val_loss: 907.9250 - _timestamp: 1655220047.0000 - _runtime: 10178.0000\n",
      "Epoch 384/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 268.4862 - val_loss: 828.2832 - _timestamp: 1655220065.0000 - _runtime: 10196.0000\n",
      "Epoch 385/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 245.8317 - val_loss: 970.1574 - _timestamp: 1655220083.0000 - _runtime: 10214.0000\n",
      "Epoch 386/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 239.3801 - val_loss: 358.0885 - _timestamp: 1655220101.0000 - _runtime: 10232.0000\n",
      "Epoch 387/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 246.6496 - val_loss: 824.1654 - _timestamp: 1655220119.0000 - _runtime: 10250.0000\n",
      "Epoch 388/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 234.0382 - val_loss: 724.8975 - _timestamp: 1655220137.0000 - _runtime: 10268.0000\n",
      "Epoch 389/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 249.6637 - val_loss: 424.4541 - _timestamp: 1655220155.0000 - _runtime: 10286.0000\n",
      "Epoch 390/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 246.8922 - val_loss: 732.0121 - _timestamp: 1655220173.0000 - _runtime: 10304.0000\n",
      "Epoch 391/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 229.0901 - val_loss: 1761.1538 - _timestamp: 1655220191.0000 - _runtime: 10322.0000\n",
      "Epoch 392/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 227.6932 - val_loss: 962.5541 - _timestamp: 1655220209.0000 - _runtime: 10340.0000\n",
      "Epoch 393/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 247.3199 - val_loss: 1348.4058 - _timestamp: 1655220226.0000 - _runtime: 10357.0000\n",
      "Epoch 394/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 262.1791 - val_loss: 1375.7506 - _timestamp: 1655220245.0000 - _runtime: 10376.0000\n",
      "Epoch 395/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 218.6115 - val_loss: 384.2510 - _timestamp: 1655220263.0000 - _runtime: 10394.0000\n",
      "Epoch 396/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 18s 36ms/step - loss: 239.2872 - val_loss: 368.8506 - _timestamp: 1655220281.0000 - _runtime: 10412.0000\n",
      "Epoch 397/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 251.2052 - val_loss: 1342.5211 - _timestamp: 1655220301.0000 - _runtime: 10432.0000\n",
      "Epoch 398/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 236.4092 - val_loss: 750.3525 - _timestamp: 1655220319.0000 - _runtime: 10450.0000\n",
      "Epoch 399/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 233.3295 - val_loss: 297.1609 - _timestamp: 1655220337.0000 - _runtime: 10468.0000\n",
      "Epoch 400/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 235.6707 - val_loss: 541.7075 - _timestamp: 1655220355.0000 - _runtime: 10486.0000\n",
      "Epoch 401/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 234.3490 - val_loss: 979.9847 - _timestamp: 1655220373.0000 - _runtime: 10504.0000\n",
      "Epoch 402/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 251.8067 - val_loss: 630.1972 - _timestamp: 1655220391.0000 - _runtime: 10522.0000\n",
      "Epoch 403/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 253.2094 - val_loss: 921.0307 - _timestamp: 1655220409.0000 - _runtime: 10540.0000\n",
      "Epoch 404/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.8503 - val_loss: 380.5602 - _timestamp: 1655220427.0000 - _runtime: 10558.0000\n",
      "Epoch 405/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.2590 - val_loss: 1504.5583 - _timestamp: 1655220445.0000 - _runtime: 10576.0000\n",
      "Epoch 406/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 252.3111 - val_loss: 1444.5492 - _timestamp: 1655220463.0000 - _runtime: 10594.0000\n",
      "Epoch 407/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 245.7130 - val_loss: 730.4772 - _timestamp: 1655220481.0000 - _runtime: 10612.0000\n",
      "Epoch 408/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 237.6009 - val_loss: 532.3589 - _timestamp: 1655220499.0000 - _runtime: 10630.0000\n",
      "Epoch 409/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 246.0600 - val_loss: 714.2888 - _timestamp: 1655220517.0000 - _runtime: 10648.0000\n",
      "Epoch 410/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 231.4274 - val_loss: 434.5063 - _timestamp: 1655220535.0000 - _runtime: 10666.0000\n",
      "Epoch 411/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 255.4592 - val_loss: 396.9749 - _timestamp: 1655220553.0000 - _runtime: 10684.0000\n",
      "Epoch 412/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 241.5242 - val_loss: 521.6397 - _timestamp: 1655220571.0000 - _runtime: 10702.0000\n",
      "Epoch 413/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 237.0421 - val_loss: 1405.9159 - _timestamp: 1655220590.0000 - _runtime: 10721.0000\n",
      "Epoch 414/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 238.4537 - val_loss: 773.6641 - _timestamp: 1655220608.0000 - _runtime: 10739.0000\n",
      "Epoch 415/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 225.7593 - val_loss: 1061.0337 - _timestamp: 1655220626.0000 - _runtime: 10757.0000\n",
      "Epoch 416/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 237.4737 - val_loss: 1008.4298 - _timestamp: 1655220644.0000 - _runtime: 10775.0000\n",
      "Epoch 417/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 244.4495 - val_loss: 1378.1304 - _timestamp: 1655220662.0000 - _runtime: 10793.0000\n",
      "Epoch 418/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 225.8785 - val_loss: 1363.3254 - _timestamp: 1655220680.0000 - _runtime: 10811.0000\n",
      "Epoch 419/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 254.4160 - val_loss: 1241.3167 - _timestamp: 1655220698.0000 - _runtime: 10829.0000\n",
      "Epoch 420/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.4542 - val_loss: 313.9403 - _timestamp: 1655220716.0000 - _runtime: 10847.0000\n",
      "Epoch 421/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 248.2257 - val_loss: 1297.2222 - _timestamp: 1655220734.0000 - _runtime: 10865.0000\n",
      "Epoch 422/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.9311 - val_loss: 898.6047 - _timestamp: 1655220752.0000 - _runtime: 10883.0000\n",
      "Epoch 423/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 235.5865"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 221s 450ms/step - loss: 235.5865 - val_loss: 169.4671 - _timestamp: 1655220770.0000 - _runtime: 10901.0000\n",
      "Epoch 424/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 226.8380 - val_loss: 652.8589 - _timestamp: 1655220991.0000 - _runtime: 11122.0000\n",
      "Epoch 425/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 246.7977 - val_loss: 740.8077 - _timestamp: 1655221008.0000 - _runtime: 11139.0000\n",
      "Epoch 426/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 260.0063 - val_loss: 465.9569 - _timestamp: 1655221026.0000 - _runtime: 11157.0000\n",
      "Epoch 427/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 241.4619 - val_loss: 563.0887 - _timestamp: 1655221044.0000 - _runtime: 11175.0000\n",
      "Epoch 428/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 238.8003 - val_loss: 1297.4713 - _timestamp: 1655221061.0000 - _runtime: 11192.0000\n",
      "Epoch 429/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 231.3238 - val_loss: 979.0544 - _timestamp: 1655221079.0000 - _runtime: 11210.0000\n",
      "Epoch 430/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.3363 - val_loss: 492.7097 - _timestamp: 1655221097.0000 - _runtime: 11228.0000\n",
      "Epoch 431/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 243.2057 - val_loss: 828.7321 - _timestamp: 1655221115.0000 - _runtime: 11246.0000\n",
      "Epoch 432/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 232.8844 - val_loss: 1804.7817 - _timestamp: 1655221132.0000 - _runtime: 11263.0000\n",
      "Epoch 433/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 227.1315 - val_loss: 741.9135 - _timestamp: 1655221150.0000 - _runtime: 11281.0000\n",
      "Epoch 434/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 240.3284 - val_loss: 639.1021 - _timestamp: 1655221168.0000 - _runtime: 11299.0000\n",
      "Epoch 435/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.0844 - val_loss: 851.7825 - _timestamp: 1655221185.0000 - _runtime: 11316.0000\n",
      "Epoch 436/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.9959 - val_loss: 1740.0024 - _timestamp: 1655221203.0000 - _runtime: 11334.0000\n",
      "Epoch 437/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 245.8900 - val_loss: 866.9640 - _timestamp: 1655221221.0000 - _runtime: 11352.0000\n",
      "Epoch 438/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 239.6863 - val_loss: 1431.5220 - _timestamp: 1655221239.0000 - _runtime: 11370.0000\n",
      "Epoch 439/1000\n",
      "491/491 [==============================] - 19s 40ms/step - loss: 236.6883 - val_loss: 428.9844 - _timestamp: 1655221258.0000 - _runtime: 11389.0000\n",
      "Epoch 440/1000\n",
      "491/491 [==============================] - 23s 47ms/step - loss: 261.1397 - val_loss: 1345.6486 - _timestamp: 1655221281.0000 - _runtime: 11412.0000\n",
      "Epoch 441/1000\n",
      "491/491 [==============================] - 24s 48ms/step - loss: 236.4308 - val_loss: 880.4677 - _timestamp: 1655221323.0000 - _runtime: 11454.0000\n",
      "Epoch 442/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 243.4242 - val_loss: 2260.7434 - _timestamp: 1655221365.0000 - _runtime: 11496.0000\n",
      "Epoch 443/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 230.7154 - val_loss: 409.1862 - _timestamp: 1655221390.0000 - _runtime: 11521.0000\n",
      "Epoch 444/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 222.4758 - val_loss: 623.7584 - _timestamp: 1655221415.0000 - _runtime: 11546.0000\n",
      "Epoch 445/1000\n",
      "491/491 [==============================] - 26s 52ms/step - loss: 247.5836 - val_loss: 1122.3002 - _timestamp: 1655221441.0000 - _runtime: 11572.0000\n",
      "Epoch 446/1000\n",
      "491/491 [==============================] - 23s 48ms/step - loss: 229.6792 - val_loss: 1578.6727 - _timestamp: 1655221464.0000 - _runtime: 11595.0000\n",
      "Epoch 447/1000\n",
      "491/491 [==============================] - 24s 49ms/step - loss: 224.6982 - val_loss: 1337.2220 - _timestamp: 1655221488.0000 - _runtime: 11619.0000\n",
      "Epoch 448/1000\n",
      "491/491 [==============================] - 28s 57ms/step - loss: 246.6269 - val_loss: 1883.4841 - _timestamp: 1655221517.0000 - _runtime: 11648.0000\n",
      "Epoch 449/1000\n",
      "491/491 [==============================] - 29s 59ms/step - loss: 242.5938 - val_loss: 726.2802 - _timestamp: 1655221546.0000 - _runtime: 11677.0000\n",
      "Epoch 450/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 227.4294 - val_loss: 1486.8062 - _timestamp: 1655221574.0000 - _runtime: 11705.0000\n",
      "Epoch 451/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 228.5591 - val_loss: 1409.2194 - _timestamp: 1655221601.0000 - _runtime: 11732.0000\n",
      "Epoch 452/1000\n",
      "491/491 [==============================] - 27s 54ms/step - loss: 256.3149 - val_loss: 910.3085 - _timestamp: 1655221628.0000 - _runtime: 11759.0000\n",
      "Epoch 453/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 241.0470 - val_loss: 1732.4187 - _timestamp: 1655221655.0000 - _runtime: 11786.0000\n",
      "Epoch 454/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 218.0593 - val_loss: 1131.9501 - _timestamp: 1655221682.0000 - _runtime: 11813.0000\n",
      "Epoch 455/1000\n",
      "491/491 [==============================] - 27s 54ms/step - loss: 233.4579 - val_loss: 1005.5311 - _timestamp: 1655221709.0000 - _runtime: 11840.0000\n",
      "Epoch 456/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 230.7413 - val_loss: 1435.4402 - _timestamp: 1655221751.0000 - _runtime: 11882.0000\n",
      "Epoch 457/1000\n",
      "491/491 [==============================] - 27s 56ms/step - loss: 245.5431 - val_loss: 2879.9531 - _timestamp: 1655221778.0000 - _runtime: 11909.0000\n",
      "Epoch 458/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 250.7761 - val_loss: 366.1172 - _timestamp: 1655221819.0000 - _runtime: 11950.0000\n",
      "Epoch 459/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 228.1019 - val_loss: 305.4675 - _timestamp: 1655221846.0000 - _runtime: 11977.0000\n",
      "Epoch 460/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 228.1066 - val_loss: 1045.3500 - _timestamp: 1655221864.0000 - _runtime: 11995.0000\n",
      "Epoch 461/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 228.9538 - val_loss: 860.5304 - _timestamp: 1655221883.0000 - _runtime: 12014.0000\n",
      "Epoch 462/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 237.1244 - val_loss: 1552.3525 - _timestamp: 1655221901.0000 - _runtime: 12032.0000\n",
      "Epoch 463/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.3430 - val_loss: 772.7042 - _timestamp: 1655221921.0000 - _runtime: 12052.0000\n",
      "Epoch 464/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 232.3793 - val_loss: 320.2937 - _timestamp: 1655221939.0000 - _runtime: 12070.0000\n",
      "Epoch 465/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 239.7495 - val_loss: 1076.9301 - _timestamp: 1655221958.0000 - _runtime: 12089.0000\n",
      "Epoch 466/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 217.4008 - val_loss: 1306.6791 - _timestamp: 1655221976.0000 - _runtime: 12107.0000\n",
      "Epoch 467/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 248.6593 - val_loss: 2077.6758 - _timestamp: 1655221994.0000 - _runtime: 12125.0000\n",
      "Epoch 468/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 218.8513 - val_loss: 349.0849 - _timestamp: 1655222012.0000 - _runtime: 12143.0000\n",
      "Epoch 469/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 217.8065 - val_loss: 488.3930 - _timestamp: 1655222030.0000 - _runtime: 12161.0000\n",
      "Epoch 470/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 221.7269 - val_loss: 1028.7296 - _timestamp: 1655222048.0000 - _runtime: 12179.0000\n",
      "Epoch 471/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 234.4892 - val_loss: 1561.2859 - _timestamp: 1655222066.0000 - _runtime: 12197.0000\n",
      "Epoch 472/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 216.9042 - val_loss: 1335.2106 - _timestamp: 1655222084.0000 - _runtime: 12215.0000\n",
      "Epoch 473/1000\n",
      "491/491 [==============================] - 22s 44ms/step - loss: 244.2473 - val_loss: 660.6140 - _timestamp: 1655222106.0000 - _runtime: 12237.0000\n",
      "Epoch 474/1000\n",
      "491/491 [==============================] - 23s 47ms/step - loss: 236.8136 - val_loss: 2366.3804 - _timestamp: 1655222130.0000 - _runtime: 12261.0000\n",
      "Epoch 475/1000\n",
      "491/491 [==============================] - 25s 50ms/step - loss: 249.8731 - val_loss: 1344.9391 - _timestamp: 1655222154.0000 - _runtime: 12285.0000\n",
      "Epoch 476/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 221.0092 - val_loss: 611.2161 - _timestamp: 1655222175.0000 - _runtime: 12306.0000\n",
      "Epoch 477/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.9722 - val_loss: 449.0672 - _timestamp: 1655222193.0000 - _runtime: 12324.0000\n",
      "Epoch 478/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.4238 - val_loss: 1541.2456 - _timestamp: 1655222212.0000 - _runtime: 12343.0000\n",
      "Epoch 479/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 221.6973 - val_loss: 1238.2863 - _timestamp: 1655222230.0000 - _runtime: 12361.0000\n",
      "Epoch 480/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 243.3754 - val_loss: 900.9999 - _timestamp: 1655222248.0000 - _runtime: 12379.0000\n",
      "Epoch 481/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.9187 - val_loss: 471.3044 - _timestamp: 1655222268.0000 - _runtime: 12399.0000\n",
      "Epoch 482/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 240.4980 - val_loss: 370.1645 - _timestamp: 1655222286.0000 - _runtime: 12417.0000\n",
      "Epoch 483/1000\n",
      "491/491 [==============================] - 20s 41ms/step - loss: 222.8212 - val_loss: 1207.2319 - _timestamp: 1655222307.0000 - _runtime: 12438.0000\n",
      "Epoch 484/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 237.1793 - val_loss: 701.6641 - _timestamp: 1655222334.0000 - _runtime: 12465.0000\n",
      "Epoch 485/1000\n",
      "491/491 [==============================] - 27s 54ms/step - loss: 220.4687 - val_loss: 1247.6892 - _timestamp: 1655222361.0000 - _runtime: 12492.0000\n",
      "Epoch 486/1000\n",
      "491/491 [==============================] - 27s 55ms/step - loss: 224.1239 - val_loss: 417.0411 - _timestamp: 1655222402.0000 - _runtime: 12533.0000\n",
      "Epoch 487/1000\n",
      "491/491 [==============================] - 26s 53ms/step - loss: 227.6717 - val_loss: 960.9658 - _timestamp: 1655222428.0000 - _runtime: 12559.0000\n",
      "Epoch 488/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 230.2230 - val_loss: 720.4386 - _timestamp: 1655222447.0000 - _runtime: 12578.0000\n",
      "Epoch 489/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 226.6928 - val_loss: 1075.7264 - _timestamp: 1655222466.0000 - _runtime: 12597.0000\n",
      "Epoch 490/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 229.7510 - val_loss: 1854.1204 - _timestamp: 1655222485.0000 - _runtime: 12616.0000\n",
      "Epoch 491/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 228.5692 - val_loss: 674.1463 - _timestamp: 1655222504.0000 - _runtime: 12635.0000\n",
      "Epoch 492/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 213.6775 - val_loss: 992.7646 - _timestamp: 1655222522.0000 - _runtime: 12653.0000\n",
      "Epoch 493/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 237.4444 - val_loss: 757.7718 - _timestamp: 1655222541.0000 - _runtime: 12672.0000\n",
      "Epoch 494/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 244.4437 - val_loss: 444.7147 - _timestamp: 1655222560.0000 - _runtime: 12691.0000\n",
      "Epoch 495/1000\n",
      "491/491 [==============================] - 19s 40ms/step - loss: 222.3716 - val_loss: 627.4031 - _timestamp: 1655222580.0000 - _runtime: 12711.0000\n",
      "Epoch 496/1000\n",
      "491/491 [==============================] - 20s 40ms/step - loss: 213.3620 - val_loss: 1924.4111 - _timestamp: 1655222600.0000 - _runtime: 12731.0000\n",
      "Epoch 497/1000\n",
      "491/491 [==============================] - 22s 45ms/step - loss: 241.8215 - val_loss: 582.1543 - _timestamp: 1655222622.0000 - _runtime: 12753.0000\n",
      "Epoch 498/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 237.4431 - val_loss: 1524.4718 - _timestamp: 1655222643.0000 - _runtime: 12774.0000\n",
      "Epoch 499/1000\n",
      "491/491 [==============================] - 22s 46ms/step - loss: 235.0277 - val_loss: 851.1237 - _timestamp: 1655222666.0000 - _runtime: 12797.0000\n",
      "Epoch 500/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 235.0706 - val_loss: 664.0760 - _timestamp: 1655222685.0000 - _runtime: 12816.0000\n",
      "Epoch 501/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 235.5300 - val_loss: 812.8943 - _timestamp: 1655222704.0000 - _runtime: 12835.0000\n",
      "Epoch 502/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 224.6251 - val_loss: 984.3073 - _timestamp: 1655222724.0000 - _runtime: 12855.0000\n",
      "Epoch 503/1000\n",
      "491/491 [==============================] - 21s 42ms/step - loss: 238.5129 - val_loss: 674.0468 - _timestamp: 1655222745.0000 - _runtime: 12876.0000\n",
      "Epoch 504/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 226.7643 - val_loss: 1029.8533 - _timestamp: 1655222764.0000 - _runtime: 12895.0000\n",
      "Epoch 505/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 221.7339 - val_loss: 676.9872 - _timestamp: 1655222783.0000 - _runtime: 12914.0000\n",
      "Epoch 506/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 231.1724 - val_loss: 1223.1321 - _timestamp: 1655222802.0000 - _runtime: 12933.0000\n",
      "Epoch 507/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 218.9576 - val_loss: 1285.0695 - _timestamp: 1655222821.0000 - _runtime: 12952.0000\n",
      "Epoch 508/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 226.0641 - val_loss: 1362.9742 - _timestamp: 1655222839.0000 - _runtime: 12970.0000\n",
      "Epoch 509/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 209.0775 - val_loss: 2254.3970 - _timestamp: 1655222858.0000 - _runtime: 12989.0000\n",
      "Epoch 510/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 224.8368 - val_loss: 269.0464 - _timestamp: 1655222877.0000 - _runtime: 13008.0000\n",
      "Epoch 511/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 220.4867 - val_loss: 1150.0671 - _timestamp: 1655222896.0000 - _runtime: 13027.0000\n",
      "Epoch 512/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 215.7021 - val_loss: 749.9504 - _timestamp: 1655222915.0000 - _runtime: 13046.0000\n",
      "Epoch 513/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 216.3934 - val_loss: 588.8444 - _timestamp: 1655222934.0000 - _runtime: 13065.0000\n",
      "Epoch 514/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 229.8376 - val_loss: 194.3510 - _timestamp: 1655222952.0000 - _runtime: 13083.0000\n",
      "Epoch 515/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 231.8697 - val_loss: 2426.7554 - _timestamp: 1655222971.0000 - _runtime: 13102.0000\n",
      "Epoch 516/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 237.1233 - val_loss: 296.0340 - _timestamp: 1655222990.0000 - _runtime: 13121.0000\n",
      "Epoch 517/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 234.6852 - val_loss: 1003.8505 - _timestamp: 1655223009.0000 - _runtime: 13140.0000\n",
      "Epoch 518/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 225.7434 - val_loss: 830.2761 - _timestamp: 1655223028.0000 - _runtime: 13159.0000\n",
      "Epoch 519/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 238.2930 - val_loss: 1820.1514 - _timestamp: 1655223047.0000 - _runtime: 13178.0000\n",
      "Epoch 520/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 232.9756 - val_loss: 1514.3879 - _timestamp: 1655223065.0000 - _runtime: 13196.0000\n",
      "Epoch 521/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 232.3638 - val_loss: 407.7931 - _timestamp: 1655223086.0000 - _runtime: 13217.0000\n",
      "Epoch 522/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 213.3114 - val_loss: 752.4160 - _timestamp: 1655223105.0000 - _runtime: 13236.0000\n",
      "Epoch 523/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 19s 38ms/step - loss: 203.1076 - val_loss: 475.6894 - _timestamp: 1655223124.0000 - _runtime: 13255.0000\n",
      "Epoch 524/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 217.3741 - val_loss: 226.1270 - _timestamp: 1655223143.0000 - _runtime: 13274.0000\n",
      "Epoch 525/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 256.7719 - val_loss: 661.6485 - _timestamp: 1655223162.0000 - _runtime: 13293.0000\n",
      "Epoch 526/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 227.2138 - val_loss: 1137.7538 - _timestamp: 1655223180.0000 - _runtime: 13311.0000\n",
      "Epoch 527/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 237.7735 - val_loss: 1028.9076 - _timestamp: 1655223199.0000 - _runtime: 13330.0000\n",
      "Epoch 528/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 228.5715 - val_loss: 708.0255 - _timestamp: 1655223218.0000 - _runtime: 13349.0000\n",
      "Epoch 529/1000\n",
      "491/491 [==============================] - 19s 38ms/step - loss: 227.8863 - val_loss: 765.1674 - _timestamp: 1655223237.0000 - _runtime: 13368.0000\n",
      "Epoch 530/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 238.1219 - val_loss: 1818.4921 - _timestamp: 1655223256.0000 - _runtime: 13387.0000\n",
      "Epoch 531/1000\n",
      "491/491 [==============================] - 21s 43ms/step - loss: 225.1904 - val_loss: 469.4782 - _timestamp: 1655223277.0000 - _runtime: 13408.0000\n",
      "Epoch 532/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 219.8411 - val_loss: 1567.2393 - _timestamp: 1655223297.0000 - _runtime: 13428.0000\n",
      "Epoch 533/1000\n",
      "491/491 [==============================] - 19s 39ms/step - loss: 224.0569 - val_loss: 1106.2814 - _timestamp: 1655223316.0000 - _runtime: 13447.0000\n",
      "Epoch 534/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 215.2385 - val_loss: 659.7812 - _timestamp: 1655223334.0000 - _runtime: 13465.0000\n",
      "Epoch 535/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.1293 - val_loss: 646.0594 - _timestamp: 1655223351.0000 - _runtime: 13482.0000\n",
      "Epoch 536/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 254.7976 - val_loss: 406.2249 - _timestamp: 1655223369.0000 - _runtime: 13500.0000\n",
      "Epoch 537/1000\n",
      "408/491 [=======================>......] - ETA: 2s - loss: 255.3135"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 18s 36ms/step - loss: 230.9583 - val_loss: 844.8167 - _timestamp: 1655223387.0000 - _runtime: 13518.0000\n",
      "Epoch 538/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 214.5613 - val_loss: 2838.0898 - _timestamp: 1655223404.0000 - _runtime: 13535.0000\n",
      "Epoch 539/1000\n",
      "491/491 [==============================] - 17s 35ms/step - loss: 234.9707 - val_loss: 708.0744 - _timestamp: 1655223422.0000 - _runtime: 13553.0000\n",
      "Epoch 540/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 223.1193 - val_loss: 1727.3994 - _timestamp: 1655223440.0000 - _runtime: 13571.0000\n",
      "Epoch 541/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 227.9631 - val_loss: 1250.8901 - _timestamp: 1655223457.0000 - _runtime: 13588.0000\n",
      "Epoch 542/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 222.9302 - val_loss: 1298.8589 - _timestamp: 1655223475.0000 - _runtime: 13606.0000\n",
      "Epoch 543/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.7930 - val_loss: 626.6533 - _timestamp: 1655223493.0000 - _runtime: 13624.0000\n",
      "Epoch 544/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 218.1080 - val_loss: 409.5818 - _timestamp: 1655223511.0000 - _runtime: 13642.0000\n",
      "Epoch 545/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.9436 - val_loss: 1448.0876 - _timestamp: 1655223529.0000 - _runtime: 13660.0000\n",
      "Epoch 546/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 219.0103 - val_loss: 967.1385 - _timestamp: 1655223546.0000 - _runtime: 13677.0000\n",
      "Epoch 547/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 210.7301 - val_loss: 377.9513 - _timestamp: 1655223564.0000 - _runtime: 13695.0000\n",
      "Epoch 548/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.8491 - val_loss: 2343.2979 - _timestamp: 1655223582.0000 - _runtime: 13713.0000\n",
      "Epoch 549/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 234.8311 - val_loss: 1068.1523 - _timestamp: 1655223600.0000 - _runtime: 13731.0000\n",
      "Epoch 550/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 236.1078 - val_loss: 322.8021 - _timestamp: 1655223618.0000 - _runtime: 13749.0000\n",
      "Epoch 551/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 233.1820 - val_loss: 1233.9719 - _timestamp: 1655223635.0000 - _runtime: 13766.0000\n",
      "Epoch 552/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 227.4521 - val_loss: 313.3475 - _timestamp: 1655223653.0000 - _runtime: 13784.0000\n",
      "Epoch 553/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.9346 - val_loss: 427.2458 - _timestamp: 1655223671.0000 - _runtime: 13802.0000\n",
      "Epoch 554/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.4853 - val_loss: 335.7371 - _timestamp: 1655223689.0000 - _runtime: 13820.0000\n",
      "Epoch 555/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 218.0629 - val_loss: 1940.8151 - _timestamp: 1655223707.0000 - _runtime: 13838.0000\n",
      "Epoch 556/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 236.3913 - val_loss: 1677.8926 - _timestamp: 1655223724.0000 - _runtime: 13855.0000\n",
      "Epoch 557/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.5762 - val_loss: 809.3300 - _timestamp: 1655223742.0000 - _runtime: 13873.0000\n",
      "Epoch 558/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 235.6106 - val_loss: 1369.5009 - _timestamp: 1655223760.0000 - _runtime: 13891.0000\n",
      "Epoch 559/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 221.7316 - val_loss: 853.2994 - _timestamp: 1655223778.0000 - _runtime: 13909.0000\n",
      "Epoch 560/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 216.1280 - val_loss: 758.0766 - _timestamp: 1655223796.0000 - _runtime: 13927.0000\n",
      "Epoch 561/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.0387 - val_loss: 1147.6041 - _timestamp: 1655223813.0000 - _runtime: 13944.0000\n",
      "Epoch 562/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 424.2027 - val_loss: 2906.0076 - _timestamp: 1655223831.0000 - _runtime: 13962.0000\n",
      "Epoch 563/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 265.5294 - val_loss: 894.6460 - _timestamp: 1655223849.0000 - _runtime: 13980.0000\n",
      "Epoch 564/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 241.3608 - val_loss: 357.1281 - _timestamp: 1655223866.0000 - _runtime: 13997.0000\n",
      "Epoch 565/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 247.0505 - val_loss: 991.5788 - _timestamp: 1655223884.0000 - _runtime: 14015.0000\n",
      "Epoch 566/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.0839 - val_loss: 851.4713 - _timestamp: 1655223902.0000 - _runtime: 14033.0000\n",
      "Epoch 567/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.9077 - val_loss: 292.0715 - _timestamp: 1655223920.0000 - _runtime: 14051.0000\n",
      "Epoch 568/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.2275 - val_loss: 1154.4752 - _timestamp: 1655223938.0000 - _runtime: 14069.0000\n",
      "Epoch 569/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.5410 - val_loss: 884.7459 - _timestamp: 1655223955.0000 - _runtime: 14086.0000\n",
      "Epoch 570/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.4862 - val_loss: 1010.0182 - _timestamp: 1655223973.0000 - _runtime: 14104.0000\n",
      "Epoch 571/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.2728 - val_loss: 1131.0215 - _timestamp: 1655223991.0000 - _runtime: 14122.0000\n",
      "Epoch 572/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 229.0499 - val_loss: 828.8466 - _timestamp: 1655224009.0000 - _runtime: 14140.0000\n",
      "Epoch 573/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 218.5328 - val_loss: 594.1149 - _timestamp: 1655224026.0000 - _runtime: 14157.0000\n",
      "Epoch 574/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 201.5430 - val_loss: 354.8209 - _timestamp: 1655224044.0000 - _runtime: 14175.0000\n",
      "Epoch 575/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.5918 - val_loss: 872.3879 - _timestamp: 1655224062.0000 - _runtime: 14193.0000\n",
      "Epoch 576/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 242.6980 - val_loss: 1714.3346 - _timestamp: 1655224080.0000 - _runtime: 14211.0000\n",
      "Epoch 577/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 231.1405 - val_loss: 890.0267 - _timestamp: 1655224097.0000 - _runtime: 14228.0000\n",
      "Epoch 578/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.4318 - val_loss: 1129.7483 - _timestamp: 1655224115.0000 - _runtime: 14246.0000\n",
      "Epoch 579/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.8449 - val_loss: 539.7941 - _timestamp: 1655224133.0000 - _runtime: 14264.0000\n",
      "Epoch 580/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.9159 - val_loss: 1107.1920 - _timestamp: 1655224151.0000 - _runtime: 14282.0000\n",
      "Epoch 581/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 222.9697 - val_loss: 701.7416 - _timestamp: 1655224168.0000 - _runtime: 14299.0000\n",
      "Epoch 582/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.5148 - val_loss: 2295.5159 - _timestamp: 1655224186.0000 - _runtime: 14317.0000\n",
      "Epoch 583/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.3830 - val_loss: 268.3676 - _timestamp: 1655224204.0000 - _runtime: 14335.0000\n",
      "Epoch 584/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 208.7683 - val_loss: 368.6256 - _timestamp: 1655224222.0000 - _runtime: 14353.0000\n",
      "Epoch 585/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 230.9592 - val_loss: 1604.5260 - _timestamp: 1655224239.0000 - _runtime: 14370.0000\n",
      "Epoch 586/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 220.7773 - val_loss: 1331.1179 - _timestamp: 1655224257.0000 - _runtime: 14388.0000\n",
      "Epoch 587/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 222.1981 - val_loss: 877.2725 - _timestamp: 1655224275.0000 - _runtime: 14406.0000\n",
      "Epoch 588/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 220.0112 - val_loss: 438.1443 - _timestamp: 1655224293.0000 - _runtime: 14424.0000\n",
      "Epoch 589/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 222.5630 - val_loss: 1058.2708 - _timestamp: 1655224310.0000 - _runtime: 14441.0000\n",
      "Epoch 590/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.9441 - val_loss: 507.9401 - _timestamp: 1655224328.0000 - _runtime: 14459.0000\n",
      "Epoch 591/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.4341 - val_loss: 1242.9780 - _timestamp: 1655224346.0000 - _runtime: 14477.0000\n",
      "Epoch 592/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 227.3259 - val_loss: 717.8130 - _timestamp: 1655224364.0000 - _runtime: 14495.0000\n",
      "Epoch 593/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.1420 - val_loss: 1731.5991 - _timestamp: 1655224381.0000 - _runtime: 14512.0000\n",
      "Epoch 594/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 206.0103 - val_loss: 670.6846 - _timestamp: 1655224399.0000 - _runtime: 14530.0000\n",
      "Epoch 595/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.6607 - val_loss: 1040.0625 - _timestamp: 1655224417.0000 - _runtime: 14548.0000\n",
      "Epoch 596/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 234.3176 - val_loss: 781.5539 - _timestamp: 1655224434.0000 - _runtime: 14565.0000\n",
      "Epoch 597/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 218.1825 - val_loss: 1107.1096 - _timestamp: 1655224452.0000 - _runtime: 14583.0000\n",
      "Epoch 598/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.6324 - val_loss: 1155.2850 - _timestamp: 1655224470.0000 - _runtime: 14601.0000\n",
      "Epoch 599/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 236.9781 - val_loss: 1661.5854 - _timestamp: 1655224488.0000 - _runtime: 14619.0000\n",
      "Epoch 600/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.8338 - val_loss: 1578.9514 - _timestamp: 1655224505.0000 - _runtime: 14636.0000\n",
      "Epoch 601/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.5252 - val_loss: 1387.2930 - _timestamp: 1655224523.0000 - _runtime: 14654.0000\n",
      "Epoch 602/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 216.8477 - val_loss: 1141.9570 - _timestamp: 1655224541.0000 - _runtime: 14672.0000\n",
      "Epoch 603/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.9763 - val_loss: 532.7774 - _timestamp: 1655224559.0000 - _runtime: 14690.0000\n",
      "Epoch 604/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.5091 - val_loss: 530.1680 - _timestamp: 1655224576.0000 - _runtime: 14707.0000\n",
      "Epoch 605/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.5021 - val_loss: 1202.7424 - _timestamp: 1655224594.0000 - _runtime: 14725.0000\n",
      "Epoch 606/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 198.4117 - val_loss: 955.0652 - _timestamp: 1655224612.0000 - _runtime: 14743.0000\n",
      "Epoch 607/1000\n",
      "491/491 [==============================] - ETA: 0s - loss: 210.6549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses, lstm_cell_1_layer_call_fn, lstm_cell_1_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ../model/demand_forecasting/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa67a37a100> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679fb3d60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7fa679f2ba60> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 83s 169ms/step - loss: 210.6549 - val_loss: 138.5595 - _timestamp: 1655224630.0000 - _runtime: 14761.0000\n",
      "Epoch 608/1000\n",
      "491/491 [==============================] - 23s 36ms/step - loss: 210.8791 - val_loss: 764.0000 - _timestamp: 1655224723.0000 - _runtime: 14854.0000\n",
      "Epoch 609/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 222.2538 - val_loss: 559.3895 - _timestamp: 1655224741.0000 - _runtime: 14872.0000\n",
      "Epoch 610/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 223.8761 - val_loss: 932.2905 - _timestamp: 1655224759.0000 - _runtime: 14890.0000\n",
      "Epoch 611/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.2344 - val_loss: 1158.6017 - _timestamp: 1655224776.0000 - _runtime: 14907.0000\n",
      "Epoch 612/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.4386 - val_loss: 1600.5128 - _timestamp: 1655224794.0000 - _runtime: 14925.0000\n",
      "Epoch 613/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.1051 - val_loss: 1480.7196 - _timestamp: 1655224812.0000 - _runtime: 14943.0000\n",
      "Epoch 614/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.7949 - val_loss: 226.9451 - _timestamp: 1655224830.0000 - _runtime: 14961.0000\n",
      "Epoch 615/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.1806 - val_loss: 587.4622 - _timestamp: 1655224848.0000 - _runtime: 14979.0000\n",
      "Epoch 616/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.0306 - val_loss: 927.3139 - _timestamp: 1655224865.0000 - _runtime: 14996.0000\n",
      "Epoch 617/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.0623 - val_loss: 506.6680 - _timestamp: 1655224883.0000 - _runtime: 15014.0000\n",
      "Epoch 618/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.2691 - val_loss: 909.5595 - _timestamp: 1655224901.0000 - _runtime: 15032.0000\n",
      "Epoch 619/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 210.1809 - val_loss: 953.2529 - _timestamp: 1655224919.0000 - _runtime: 15050.0000\n",
      "Epoch 620/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 208.8995 - val_loss: 414.7892 - _timestamp: 1655224936.0000 - _runtime: 15067.0000\n",
      "Epoch 621/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.1255 - val_loss: 977.5727 - _timestamp: 1655224954.0000 - _runtime: 15085.0000\n",
      "Epoch 622/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 221.7419 - val_loss: 1080.7345 - _timestamp: 1655224972.0000 - _runtime: 15103.0000\n",
      "Epoch 623/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 236.1762 - val_loss: 462.6996 - _timestamp: 1655224989.0000 - _runtime: 15120.0000\n",
      "Epoch 624/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 216.5562 - val_loss: 1180.0447 - _timestamp: 1655225007.0000 - _runtime: 15138.0000\n",
      "Epoch 625/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 209.8869 - val_loss: 1518.0886 - _timestamp: 1655225025.0000 - _runtime: 15156.0000\n",
      "Epoch 626/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.3956 - val_loss: 683.6169 - _timestamp: 1655225042.0000 - _runtime: 15173.0000\n",
      "Epoch 627/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.7591 - val_loss: 549.5176 - _timestamp: 1655225060.0000 - _runtime: 15191.0000\n",
      "Epoch 628/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 216.1302 - val_loss: 343.2156 - _timestamp: 1655225078.0000 - _runtime: 15209.0000\n",
      "Epoch 629/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 220.2830 - val_loss: 949.5892 - _timestamp: 1655225095.0000 - _runtime: 15226.0000\n",
      "Epoch 630/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.9575 - val_loss: 793.7124 - _timestamp: 1655225113.0000 - _runtime: 15244.0000\n",
      "Epoch 631/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.6303 - val_loss: 810.2332 - _timestamp: 1655225131.0000 - _runtime: 15262.0000\n",
      "Epoch 632/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 216.8891 - val_loss: 330.2615 - _timestamp: 1655225149.0000 - _runtime: 15280.0000\n",
      "Epoch 633/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 231.3989 - val_loss: 1356.3278 - _timestamp: 1655225166.0000 - _runtime: 15297.0000\n",
      "Epoch 634/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.9789 - val_loss: 1591.4210 - _timestamp: 1655225184.0000 - _runtime: 15315.0000\n",
      "Epoch 635/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 215.1030 - val_loss: 666.6248 - _timestamp: 1655225202.0000 - _runtime: 15333.0000\n",
      "Epoch 636/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.9810 - val_loss: 1438.2527 - _timestamp: 1655225220.0000 - _runtime: 15351.0000\n",
      "Epoch 637/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.1993 - val_loss: 800.1810 - _timestamp: 1655225237.0000 - _runtime: 15368.0000\n",
      "Epoch 638/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.2845 - val_loss: 1270.3483 - _timestamp: 1655225255.0000 - _runtime: 15386.0000\n",
      "Epoch 639/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 220.8530 - val_loss: 861.6030 - _timestamp: 1655225273.0000 - _runtime: 15404.0000\n",
      "Epoch 640/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.2620 - val_loss: 1580.1927 - _timestamp: 1655225290.0000 - _runtime: 15421.0000\n",
      "Epoch 641/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.8416 - val_loss: 1328.2628 - _timestamp: 1655225308.0000 - _runtime: 15439.0000\n",
      "Epoch 642/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.4553 - val_loss: 1079.9674 - _timestamp: 1655225326.0000 - _runtime: 15457.0000\n",
      "Epoch 643/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 198.4680 - val_loss: 417.6356 - _timestamp: 1655225344.0000 - _runtime: 15475.0000\n",
      "Epoch 644/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 205.5365 - val_loss: 1772.5569 - _timestamp: 1655225361.0000 - _runtime: 15492.0000\n",
      "Epoch 645/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.3983 - val_loss: 1623.8523 - _timestamp: 1655225379.0000 - _runtime: 15510.0000\n",
      "Epoch 646/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.2368 - val_loss: 977.8830 - _timestamp: 1655225397.0000 - _runtime: 15528.0000\n",
      "Epoch 647/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 201.3631 - val_loss: 1498.1976 - _timestamp: 1655225414.0000 - _runtime: 15545.0000\n",
      "Epoch 648/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.4744 - val_loss: 884.9570 - _timestamp: 1655225432.0000 - _runtime: 15563.0000\n",
      "Epoch 649/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 231.3180 - val_loss: 901.2325 - _timestamp: 1655225450.0000 - _runtime: 15581.0000\n",
      "Epoch 650/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 210.1877 - val_loss: 1317.6836 - _timestamp: 1655225468.0000 - _runtime: 15599.0000\n",
      "Epoch 651/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.4606 - val_loss: 2075.1333 - _timestamp: 1655225485.0000 - _runtime: 15616.0000\n",
      "Epoch 652/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 211.5276 - val_loss: 1799.0249 - _timestamp: 1655225503.0000 - _runtime: 15634.0000\n",
      "Epoch 653/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 242.6747 - val_loss: 581.7062 - _timestamp: 1655225521.0000 - _runtime: 15652.0000\n",
      "Epoch 654/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.2674 - val_loss: 657.0550 - _timestamp: 1655225539.0000 - _runtime: 15670.0000\n",
      "Epoch 655/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 215.8532 - val_loss: 1646.1045 - _timestamp: 1655225557.0000 - _runtime: 15688.0000\n",
      "Epoch 656/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 227.8093 - val_loss: 1126.7578 - _timestamp: 1655225574.0000 - _runtime: 15705.0000\n",
      "Epoch 657/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.7810 - val_loss: 1257.1088 - _timestamp: 1655225592.0000 - _runtime: 15723.0000\n",
      "Epoch 658/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 206.2861 - val_loss: 946.9741 - _timestamp: 1655225610.0000 - _runtime: 15741.0000\n",
      "Epoch 659/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 211.0122 - val_loss: 289.0582 - _timestamp: 1655225627.0000 - _runtime: 15758.0000\n",
      "Epoch 660/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.2368 - val_loss: 1264.9093 - _timestamp: 1655225645.0000 - _runtime: 15776.0000\n",
      "Epoch 661/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 228.4031 - val_loss: 1407.8932 - _timestamp: 1655225663.0000 - _runtime: 15794.0000\n",
      "Epoch 662/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 201.1333 - val_loss: 490.0826 - _timestamp: 1655225680.0000 - _runtime: 15811.0000\n",
      "Epoch 663/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.9421 - val_loss: 672.3420 - _timestamp: 1655225698.0000 - _runtime: 15829.0000\n",
      "Epoch 664/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 222.7242 - val_loss: 306.6187 - _timestamp: 1655225716.0000 - _runtime: 15847.0000\n",
      "Epoch 665/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.0892 - val_loss: 574.6259 - _timestamp: 1655225734.0000 - _runtime: 15865.0000\n",
      "Epoch 666/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 197.6698 - val_loss: 918.2984 - _timestamp: 1655225751.0000 - _runtime: 15882.0000\n",
      "Epoch 667/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.8066 - val_loss: 790.8753 - _timestamp: 1655225769.0000 - _runtime: 15900.0000\n",
      "Epoch 668/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.1602 - val_loss: 1353.1094 - _timestamp: 1655225787.0000 - _runtime: 15918.0000\n",
      "Epoch 669/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.6914 - val_loss: 671.8911 - _timestamp: 1655225805.0000 - _runtime: 15936.0000\n",
      "Epoch 670/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 224.1030 - val_loss: 1350.0107 - _timestamp: 1655225822.0000 - _runtime: 15953.0000\n",
      "Epoch 671/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.2555 - val_loss: 1181.4816 - _timestamp: 1655225840.0000 - _runtime: 15971.0000\n",
      "Epoch 672/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.5272 - val_loss: 1437.4930 - _timestamp: 1655225858.0000 - _runtime: 15989.0000\n",
      "Epoch 673/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.7853 - val_loss: 808.6229 - _timestamp: 1655225875.0000 - _runtime: 16006.0000\n",
      "Epoch 674/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.3242 - val_loss: 1205.0197 - _timestamp: 1655225893.0000 - _runtime: 16024.0000\n",
      "Epoch 675/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.0415 - val_loss: 925.1381 - _timestamp: 1655225911.0000 - _runtime: 16042.0000\n",
      "Epoch 676/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 231.1220 - val_loss: 892.0933 - _timestamp: 1655225929.0000 - _runtime: 16060.0000\n",
      "Epoch 677/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 211.3913 - val_loss: 1617.5033 - _timestamp: 1655225946.0000 - _runtime: 16077.0000\n",
      "Epoch 678/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 210.8191 - val_loss: 695.2931 - _timestamp: 1655225964.0000 - _runtime: 16095.0000\n",
      "Epoch 679/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.8327 - val_loss: 1515.1063 - _timestamp: 1655225982.0000 - _runtime: 16113.0000\n",
      "Epoch 680/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.2725 - val_loss: 244.1557 - _timestamp: 1655225999.0000 - _runtime: 16130.0000\n",
      "Epoch 681/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.5260 - val_loss: 340.2866 - _timestamp: 1655226017.0000 - _runtime: 16148.0000\n",
      "Epoch 682/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 200.4534 - val_loss: 741.8038 - _timestamp: 1655226035.0000 - _runtime: 16166.0000\n",
      "Epoch 683/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 233.0697 - val_loss: 178.2947 - _timestamp: 1655226053.0000 - _runtime: 16184.0000\n",
      "Epoch 684/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 200.3322 - val_loss: 1071.1686 - _timestamp: 1655226071.0000 - _runtime: 16202.0000\n",
      "Epoch 685/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.6495 - val_loss: 763.5245 - _timestamp: 1655226088.0000 - _runtime: 16219.0000\n",
      "Epoch 686/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.7840 - val_loss: 1340.3925 - _timestamp: 1655226106.0000 - _runtime: 16237.0000\n",
      "Epoch 687/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 210.3339 - val_loss: 507.3438 - _timestamp: 1655226124.0000 - _runtime: 16255.0000\n",
      "Epoch 688/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 219.9864 - val_loss: 684.7122 - _timestamp: 1655226141.0000 - _runtime: 16272.0000\n",
      "Epoch 689/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.1407 - val_loss: 917.3862 - _timestamp: 1655226159.0000 - _runtime: 16290.0000\n",
      "Epoch 690/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.8171 - val_loss: 1143.8900 - _timestamp: 1655226177.0000 - _runtime: 16308.0000\n",
      "Epoch 691/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.3345 - val_loss: 1604.0652 - _timestamp: 1655226195.0000 - _runtime: 16326.0000\n",
      "Epoch 692/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.7782 - val_loss: 639.2624 - _timestamp: 1655226212.0000 - _runtime: 16343.0000\n",
      "Epoch 693/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.7441 - val_loss: 1369.3130 - _timestamp: 1655226230.0000 - _runtime: 16361.0000\n",
      "Epoch 694/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.2933 - val_loss: 2558.9587 - _timestamp: 1655226248.0000 - _runtime: 16379.0000\n",
      "Epoch 695/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.4446 - val_loss: 1059.4998 - _timestamp: 1655226266.0000 - _runtime: 16397.0000\n",
      "Epoch 696/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 208.6299 - val_loss: 995.3015 - _timestamp: 1655226283.0000 - _runtime: 16414.0000\n",
      "Epoch 697/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 215.1169 - val_loss: 779.2407 - _timestamp: 1655226301.0000 - _runtime: 16432.0000\n",
      "Epoch 698/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 200.3410 - val_loss: 1021.9333 - _timestamp: 1655226319.0000 - _runtime: 16450.0000\n",
      "Epoch 699/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.8385 - val_loss: 233.1307 - _timestamp: 1655226337.0000 - _runtime: 16468.0000\n",
      "Epoch 700/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 202.4322 - val_loss: 698.4570 - _timestamp: 1655226355.0000 - _runtime: 16486.0000\n",
      "Epoch 701/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 208.6628 - val_loss: 640.6785 - _timestamp: 1655226372.0000 - _runtime: 16503.0000\n",
      "Epoch 702/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.5206 - val_loss: 1007.1089 - _timestamp: 1655226390.0000 - _runtime: 16521.0000\n",
      "Epoch 703/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.8689 - val_loss: 935.8995 - _timestamp: 1655226408.0000 - _runtime: 16539.0000\n",
      "Epoch 704/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 192.5971 - val_loss: 675.1982 - _timestamp: 1655226426.0000 - _runtime: 16557.0000\n",
      "Epoch 705/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 206.0388 - val_loss: 437.1169 - _timestamp: 1655226443.0000 - _runtime: 16574.0000\n",
      "Epoch 706/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.0673 - val_loss: 797.0585 - _timestamp: 1655226461.0000 - _runtime: 16592.0000\n",
      "Epoch 707/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 17s 36ms/step - loss: 221.5855 - val_loss: 533.3320 - _timestamp: 1655226479.0000 - _runtime: 16610.0000\n",
      "Epoch 708/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.1219 - val_loss: 849.6878 - _timestamp: 1655226496.0000 - _runtime: 16627.0000\n",
      "Epoch 709/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 206.0623 - val_loss: 747.7977 - _timestamp: 1655226514.0000 - _runtime: 16645.0000\n",
      "Epoch 710/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.8226 - val_loss: 1468.6024 - _timestamp: 1655226532.0000 - _runtime: 16663.0000\n",
      "Epoch 711/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 196.3568 - val_loss: 1364.8766 - _timestamp: 1655226550.0000 - _runtime: 16681.0000\n",
      "Epoch 712/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 210.4371 - val_loss: 450.8384 - _timestamp: 1655226567.0000 - _runtime: 16698.0000\n",
      "Epoch 713/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 189.3109 - val_loss: 507.7926 - _timestamp: 1655226585.0000 - _runtime: 16716.0000\n",
      "Epoch 714/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 211.7545 - val_loss: 512.7952 - _timestamp: 1655226603.0000 - _runtime: 16734.0000\n",
      "Epoch 715/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.3485 - val_loss: 2316.1318 - _timestamp: 1655226620.0000 - _runtime: 16751.0000\n",
      "Epoch 716/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.4566 - val_loss: 881.0605 - _timestamp: 1655226638.0000 - _runtime: 16769.0000\n",
      "Epoch 717/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 201.8397 - val_loss: 1548.2159 - _timestamp: 1655226656.0000 - _runtime: 16787.0000\n",
      "Epoch 718/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.4585 - val_loss: 786.0751 - _timestamp: 1655226674.0000 - _runtime: 16805.0000\n",
      "Epoch 719/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 208.0904 - val_loss: 1066.9946 - _timestamp: 1655226691.0000 - _runtime: 16822.0000\n",
      "Epoch 720/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.8755 - val_loss: 1218.6218 - _timestamp: 1655226709.0000 - _runtime: 16840.0000\n",
      "Epoch 721/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 194.0044 - val_loss: 1174.0092 - _timestamp: 1655226727.0000 - _runtime: 16858.0000\n",
      "Epoch 722/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.1502 - val_loss: 375.2615 - _timestamp: 1655226745.0000 - _runtime: 16876.0000\n",
      "Epoch 723/1000\n",
      "491/491 [==============================] - 17s 36ms/step - loss: 205.5371 - val_loss: 719.3897 - _timestamp: 1655226762.0000 - _runtime: 16893.0000\n",
      "Epoch 724/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 210.7243 - val_loss: 1395.1759 - _timestamp: 1655226780.0000 - _runtime: 16911.0000\n",
      "Epoch 725/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.8461 - val_loss: 2161.8865 - _timestamp: 1655226798.0000 - _runtime: 16929.0000\n",
      "Epoch 726/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.4147 - val_loss: 832.7012 - _timestamp: 1655226815.0000 - _runtime: 16946.0000\n",
      "Epoch 727/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 210.5405 - val_loss: 723.7097 - _timestamp: 1655226833.0000 - _runtime: 16964.0000\n",
      "Epoch 728/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 198.8979 - val_loss: 478.3918 - _timestamp: 1655226851.0000 - _runtime: 16982.0000\n",
      "Epoch 729/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 200.1770 - val_loss: 1652.9492 - _timestamp: 1655226869.0000 - _runtime: 17000.0000\n",
      "Epoch 730/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.0715 - val_loss: 1118.3021 - _timestamp: 1655226886.0000 - _runtime: 17017.0000\n",
      "Epoch 731/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 192.7639 - val_loss: 1400.4266 - _timestamp: 1655226904.0000 - _runtime: 17035.0000\n",
      "Epoch 732/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 192.7422 - val_loss: 620.3517 - _timestamp: 1655226922.0000 - _runtime: 17053.0000\n",
      "Epoch 733/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.8279 - val_loss: 589.0165 - _timestamp: 1655226940.0000 - _runtime: 17071.0000\n",
      "Epoch 734/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.5374 - val_loss: 1200.3867 - _timestamp: 1655226958.0000 - _runtime: 17089.0000\n",
      "Epoch 735/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 194.7642 - val_loss: 1093.7600 - _timestamp: 1655226975.0000 - _runtime: 17106.0000\n",
      "Epoch 736/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 202.0251 - val_loss: 1051.5234 - _timestamp: 1655226993.0000 - _runtime: 17124.0000\n",
      "Epoch 737/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 193.1102 - val_loss: 677.6104 - _timestamp: 1655227011.0000 - _runtime: 17142.0000\n",
      "Epoch 738/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 223.8365 - val_loss: 685.3091 - _timestamp: 1655227029.0000 - _runtime: 17160.0000\n",
      "Epoch 739/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 210.0136 - val_loss: 886.0009 - _timestamp: 1655227046.0000 - _runtime: 17177.0000\n",
      "Epoch 740/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 197.1592 - val_loss: 727.7422 - _timestamp: 1655227064.0000 - _runtime: 17195.0000\n",
      "Epoch 741/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 206.1445 - val_loss: 671.4442 - _timestamp: 1655227082.0000 - _runtime: 17213.0000\n",
      "Epoch 742/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 201.2610 - val_loss: 438.7224 - _timestamp: 1655227100.0000 - _runtime: 17231.0000\n",
      "Epoch 743/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 202.0776 - val_loss: 1107.5579 - _timestamp: 1655227117.0000 - _runtime: 17248.0000\n",
      "Epoch 744/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 215.4759 - val_loss: 732.7228 - _timestamp: 1655227135.0000 - _runtime: 17266.0000\n",
      "Epoch 745/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 206.7052 - val_loss: 409.5517 - _timestamp: 1655227153.0000 - _runtime: 17284.0000\n",
      "Epoch 746/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.2937 - val_loss: 269.2506 - _timestamp: 1655227171.0000 - _runtime: 17302.0000\n",
      "Epoch 747/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.4337 - val_loss: 552.0052 - _timestamp: 1655227188.0000 - _runtime: 17319.0000\n",
      "Epoch 748/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.7290 - val_loss: 797.3903 - _timestamp: 1655227206.0000 - _runtime: 17337.0000\n",
      "Epoch 749/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 189.1682 - val_loss: 1019.4169 - _timestamp: 1655227224.0000 - _runtime: 17355.0000\n",
      "Epoch 750/1000\n",
      "491/491 [==============================] - 18s 37ms/step - loss: 202.0921 - val_loss: 1278.5010 - _timestamp: 1655227242.0000 - _runtime: 17373.0000\n",
      "Epoch 751/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.8921 - val_loss: 501.3624 - _timestamp: 1655227260.0000 - _runtime: 17391.0000\n",
      "Epoch 752/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 217.4637 - val_loss: 951.2054 - _timestamp: 1655227278.0000 - _runtime: 17409.0000\n",
      "Epoch 753/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 190.0017 - val_loss: 622.8643 - _timestamp: 1655227295.0000 - _runtime: 17426.0000\n",
      "Epoch 754/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 197.1316 - val_loss: 1038.5497 - _timestamp: 1655227313.0000 - _runtime: 17444.0000\n",
      "Epoch 755/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 201.2004 - val_loss: 1267.9763 - _timestamp: 1655227331.0000 - _runtime: 17462.0000\n",
      "Epoch 756/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.2314 - val_loss: 573.3901 - _timestamp: 1655227349.0000 - _runtime: 17480.0000\n",
      "Epoch 757/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 202.3416 - val_loss: 1213.2209 - _timestamp: 1655227366.0000 - _runtime: 17497.0000\n",
      "Epoch 758/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 194.4580 - val_loss: 745.5682 - _timestamp: 1655227384.0000 - _runtime: 17515.0000\n",
      "Epoch 759/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 222.1444 - val_loss: 1124.9711 - _timestamp: 1655227402.0000 - _runtime: 17533.0000\n",
      "Epoch 760/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 192.2807 - val_loss: 1526.7863 - _timestamp: 1655227420.0000 - _runtime: 17551.0000\n",
      "Epoch 761/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 218.7003 - val_loss: 1316.7688 - _timestamp: 1655227437.0000 - _runtime: 17568.0000\n",
      "Epoch 762/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.0151 - val_loss: 1392.9784 - _timestamp: 1655227455.0000 - _runtime: 17586.0000\n",
      "Epoch 763/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.3773 - val_loss: 1116.4094 - _timestamp: 1655227473.0000 - _runtime: 17604.0000\n",
      "Epoch 764/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 215.2578 - val_loss: 1341.4047 - _timestamp: 1655227491.0000 - _runtime: 17622.0000\n",
      "Epoch 765/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 191.7204 - val_loss: 1005.4899 - _timestamp: 1655227508.0000 - _runtime: 17639.0000\n",
      "Epoch 766/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.6668 - val_loss: 863.5204 - _timestamp: 1655227526.0000 - _runtime: 17657.0000\n",
      "Epoch 767/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 195.0165 - val_loss: 1096.0936 - _timestamp: 1655227544.0000 - _runtime: 17675.0000\n",
      "Epoch 768/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.7569 - val_loss: 1244.6200 - _timestamp: 1655227562.0000 - _runtime: 17693.0000\n",
      "Epoch 769/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.4206 - val_loss: 2261.6694 - _timestamp: 1655227580.0000 - _runtime: 17711.0000\n",
      "Epoch 770/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.5935 - val_loss: 177.8174 - _timestamp: 1655227597.0000 - _runtime: 17728.0000\n",
      "Epoch 771/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 191.1353 - val_loss: 976.3882 - _timestamp: 1655227615.0000 - _runtime: 17746.0000\n",
      "Epoch 772/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 204.3238 - val_loss: 904.1930 - _timestamp: 1655227633.0000 - _runtime: 17764.0000\n",
      "Epoch 773/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 213.9129 - val_loss: 700.0360 - _timestamp: 1655227651.0000 - _runtime: 17782.0000\n",
      "Epoch 774/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 225.4079 - val_loss: 769.9950 - _timestamp: 1655227668.0000 - _runtime: 17799.0000\n",
      "Epoch 775/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 193.2529 - val_loss: 547.3400 - _timestamp: 1655227686.0000 - _runtime: 17817.0000\n",
      "Epoch 776/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.6127 - val_loss: 421.3970 - _timestamp: 1655227704.0000 - _runtime: 17835.0000\n",
      "Epoch 777/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.5919 - val_loss: 298.1248 - _timestamp: 1655227722.0000 - _runtime: 17853.0000\n",
      "Epoch 778/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.3021 - val_loss: 1490.7142 - _timestamp: 1655227739.0000 - _runtime: 17870.0000\n",
      "Epoch 779/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 212.1924 - val_loss: 642.9161 - _timestamp: 1655227757.0000 - _runtime: 17888.0000\n",
      "Epoch 780/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 198.4180 - val_loss: 981.4680 - _timestamp: 1655227775.0000 - _runtime: 17906.0000\n",
      "Epoch 781/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 202.4357 - val_loss: 1758.0858 - _timestamp: 1655227793.0000 - _runtime: 17924.0000\n",
      "Epoch 782/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 194.0730 - val_loss: 712.3911 - _timestamp: 1655227810.0000 - _runtime: 17941.0000\n",
      "Epoch 783/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 190.3666 - val_loss: 279.2084 - _timestamp: 1655227828.0000 - _runtime: 17959.0000\n",
      "Epoch 784/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 206.6739 - val_loss: 1078.6112 - _timestamp: 1655227846.0000 - _runtime: 17977.0000\n",
      "Epoch 785/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 192.1935 - val_loss: 1386.3771 - _timestamp: 1655227864.0000 - _runtime: 17995.0000\n",
      "Epoch 786/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 209.0130 - val_loss: 870.4948 - _timestamp: 1655227882.0000 - _runtime: 18013.0000\n",
      "Epoch 787/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.8159 - val_loss: 942.3992 - _timestamp: 1655227899.0000 - _runtime: 18030.0000\n",
      "Epoch 788/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.3512 - val_loss: 421.1638 - _timestamp: 1655227917.0000 - _runtime: 18048.0000\n",
      "Epoch 789/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 192.7251 - val_loss: 678.9892 - _timestamp: 1655227935.0000 - _runtime: 18066.0000\n",
      "Epoch 790/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 187.2388 - val_loss: 1655.3717 - _timestamp: 1655227953.0000 - _runtime: 18084.0000\n",
      "Epoch 791/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 200.0054 - val_loss: 1144.1859 - _timestamp: 1655227970.0000 - _runtime: 18101.0000\n",
      "Epoch 792/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.4495 - val_loss: 1096.6057 - _timestamp: 1655227988.0000 - _runtime: 18119.0000\n",
      "Epoch 793/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 193.3780 - val_loss: 405.2611 - _timestamp: 1655228006.0000 - _runtime: 18137.0000\n",
      "Epoch 794/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 200.6530 - val_loss: 1472.9359 - _timestamp: 1655228024.0000 - _runtime: 18155.0000\n",
      "Epoch 795/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 197.6734 - val_loss: 2730.5481 - _timestamp: 1655228041.0000 - _runtime: 18172.0000\n",
      "Epoch 796/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 214.7875 - val_loss: 943.1024 - _timestamp: 1655228059.0000 - _runtime: 18190.0000\n",
      "Epoch 797/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.8334 - val_loss: 849.5858 - _timestamp: 1655228077.0000 - _runtime: 18208.0000\n",
      "Epoch 798/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 201.1897 - val_loss: 867.4187 - _timestamp: 1655228095.0000 - _runtime: 18226.0000\n",
      "Epoch 799/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 205.7491 - val_loss: 1699.4431 - _timestamp: 1655228112.0000 - _runtime: 18243.0000\n",
      "Epoch 800/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 197.6999 - val_loss: 488.1164 - _timestamp: 1655228130.0000 - _runtime: 18261.0000\n",
      "Epoch 801/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 207.0742 - val_loss: 702.0040 - _timestamp: 1655228148.0000 - _runtime: 18279.0000\n",
      "Epoch 802/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 231.8877 - val_loss: 1107.1011 - _timestamp: 1655228166.0000 - _runtime: 18297.0000\n",
      "Epoch 803/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.8342 - val_loss: 339.0185 - _timestamp: 1655228184.0000 - _runtime: 18315.0000\n",
      "Epoch 804/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 199.5705 - val_loss: 1177.0435 - _timestamp: 1655228202.0000 - _runtime: 18333.0000\n",
      "Epoch 805/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.8941 - val_loss: 518.6730 - _timestamp: 1655228220.0000 - _runtime: 18351.0000\n",
      "Epoch 806/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 196.1872 - val_loss: 532.4845 - _timestamp: 1655228238.0000 - _runtime: 18369.0000\n",
      "Epoch 807/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 [==============================] - 18s 36ms/step - loss: 191.8564 - val_loss: 1806.9581 - _timestamp: 1655228255.0000 - _runtime: 18386.0000\n",
      "Epoch 808/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 198.5390 - val_loss: 523.0257 - _timestamp: 1655228273.0000 - _runtime: 18404.0000\n",
      "Epoch 809/1000\n",
      "491/491 [==============================] - 18s 36ms/step - loss: 203.8027 - val_loss: 811.7017 - _timestamp: 1655228291.0000 - _runtime: 18422.0000\n",
      "Epoch 810/1000\n",
      "170/491 [=========>....................] - ETA: 10s - loss: 276.5261"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train,\n",
    "    validation_data=valid,\n",
    "    epochs=1000,\n",
    "    callbacks=[WandbCallback(), earlystopping_cb, checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384cc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a38be",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_bruh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b6aaec",
   "metadata": {},
   "source": [
    "### Price Optimization Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5918e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_week(model, data):\n",
    "    pred = model.predict(data)[0][0]\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def forecast(model, data, prices, window_size=config.window_size, batch_size=config.batch_size, buffer_size=config.buffer_size):\n",
    "    data = data[['scaled_sales', 'scaled_price', 'sales']].copy()\n",
    "    predictions = []\n",
    "    for price in prices:\n",
    "        data = data.append({'scaled_sales':0, 'scaled_price': scaler.transform([[0, price]])[0, 1]}, ignore_index = True)\n",
    "        current_week = create_windowed_dataset(data.iloc[-(config.batch_size + config.window_size):], with_label=True)\n",
    "        next_week_sales = predict_next_week(model, current_week)\n",
    "        predictions.append(next_week_sales)\n",
    "        data['sales'].iloc[-1] = next_week_sales\n",
    "        \n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0845b7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"../model/demand_forecasting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e572e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelPipeline:\n",
    "#     def __init__(self, model, data_pipeline):\n",
    "#         self.model = model\n",
    "#         self.data_pipeline = data_pipeline\n",
    "        \n",
    "#     def predict_next_week(self, model, data):\n",
    "#         return self.model.predict(data)[0][0]\n",
    "\n",
    "#     def forecast(self, data, prices):\n",
    "#         data = self.data_pipeline.rescale(data)\n",
    "#         data = data[['scaled_sales', 'scaled_price', 'sales']].copy()\n",
    "#         predictions = []\n",
    "#         for price in prices:\n",
    "#             data = data.append({'scaled_sales':0, 'scaled_price': self.data_pipeline.scaler.transform([[0, price]])[0, 1]}, ignore_index = True)\n",
    "#             current_week = self.data_pipeline.create_windowed_dataset(data.iloc[-(self.data_pipeline.batch_size + self.data_pipeline.window_size):], with_label=True, rescale=False)\n",
    "#             next_week_sales = self.predict_next_week(self.model, current_week)\n",
    "#             predictions.append(next_week_sales)\n",
    "#             data['sales'].iloc[-1] = next_week_sales\n",
    "\n",
    "#         return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78ea3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = ModelPipeline(model, data_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc4457",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(pipeline, \"../model/demand_forecasting/pipeline.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sales = pipeline.forecast(valid, [312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355,\n",
    "                                          312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355,\n",
    "                                          312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355,\n",
    "                                          312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, \n",
    "                                         ])\n",
    "plt.plot(predicted_sales)\n",
    "print(np.mean(predicted_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c566768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No discount at all\n",
    "# predicted_sales = forecast(model, valid, [312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355,\n",
    "#                                           312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355,\n",
    "#                                           312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355,\n",
    "#                                           312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, \n",
    "#                                          ])\n",
    "plt.plot(predicted_sales)\n",
    "print(np.mean(predicted_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4afb1828",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_windowed_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# No discount at all\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predicted_sales \u001b[38;5;241m=\u001b[39m \u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m312089.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300000.685355\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(predicted_sales)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(predicted_sales))\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mforecast\u001b[0;34m(model, data, prices, window_size, batch_size, buffer_size)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m price \u001b[38;5;129;01min\u001b[39;00m prices:\n\u001b[1;32m     10\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_sales\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaled_price\u001b[39m\u001b[38;5;124m'\u001b[39m: scaler\u001b[38;5;241m.\u001b[39mtransform([[\u001b[38;5;241m0\u001b[39m, price]])[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]}, ignore_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m     current_week \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_windowed_dataset\u001b[49m(data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m-\u001b[39m(config\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mwindow_size):], with_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m     next_week_sales \u001b[38;5;241m=\u001b[39m predict_next_week(model, current_week)\n\u001b[1;32m     13\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(next_week_sales)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_windowed_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# No discount at all\n",
    "predicted_sales = forecast(model, valid, [312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 300000.685355, 300000.685355,\n",
    "                                          312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 300000.685355, 300000.685355,\n",
    "                                          312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 300000.685355, 300000.685355,\n",
    "                                          312089.685355, 312089.685355, 312089.685355, 312089.685355, 312089.685355, 300000.685355, 300000.685355,\n",
    "                                         ])\n",
    "plt.plot(predicted_sales)\n",
    "print(np.mean(predicted_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566b0d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discount on the first 2 weeks\n",
    "predicted_sales = forecast(model, valid, [0, 0, 0, 0, 0, 0, 0,\n",
    "                                          0, 0, 0, 0, 0, 0, 0,\n",
    "                                          0, 0, 0, 0, 0, 0, 0,\n",
    "                                          0, 0, 0, 0, 0, 0, 0,\n",
    "                                         ])\n",
    "plt.plot(predicted_sales)\n",
    "print(np.mean(predicted_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182f00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
